\chapter{Vícerozměrná lineární regrese}

Předpokládejme, že kromě $y_i$ máme pro~každé $i\in\hat{n}$ k dispozici také $m$ nezávislých proměnných $x_{i1},x_{i2},...,x_{im}$. Pak získáme model
$$ Y_i=\beta_0+\sum_{j=1}^m \beta_j x_{ij}+e_i,\quad i\in\hat{n},$$
kde $e_1,...,e_n$ jsou \textbf{nezávislé (nekorelované)} chyby a~$e_i\sim\NN(0,\sigma^2)$. Na~základě pozorování $(x_{i1},...,x_{im},y_i),~i\in\hat{n}$ chceme odhadnout parametr $\betab=(\beta_0,\beta_1,\dots,\beta_m)^T$ (proložení dat $m+1$ dimenzionální nadrovinou). Předpokládejme, že $n>m+1$, tj. že máme více dat než parametrů. Maticově můžeme tento stav zapsat jako
$$ \Y=(Y_1,...,Y_n)^T,\quad \Y=(y_1,...,y_n)^T,\quad \textbf{e}=(e_1,...,e_n)^T.$$
Označme 
$$ \X=\left[ \begin{array}{cccc}
1 & x_{11} & \dots & x_{1m} \\
1 & x_{21} & \dots & x_{2m} \\
1 & \vdots & \dots & \vdots \\
 \vdots& x_{n1} & \dots & x_{nm} 
\end{array}
 \right]$$ jako \textbf{matici modelu} (regresní matici, !!něco matrix, nepřečtu to???). Dostaneme tak model ve~tvaru (důležitém)
  \begin{equation}\label{the_chosen_one}
 \Y_{n\times 1}=\X_{n\times(m+1)}\betab_{(n+1)\times 1}+\textbf{e}_{bn\times 1}.
 \end{equation}
 
 Nyní budeme předpokládat, že $e_1,...,e_n$ jsou nezávislé a~$e_i \sim\NN(0\sigma^2)$, tzn. $\textbf{e}\sim\NN_n(\textbf{0},\sigma^2 I_n)$ a~$\Y\sim\NN_n(\X\beta,\sigma^2 I_n)$. 
 
 Věrohodnostní funkce je potom ve~tvaru 
 \begin{align*}
 L(\beta,\sigma^2)&=f_\pi(\Y)=\prod_{i=1}^n \frac{1}{\sqrt{2\pi\sigma^2}}\e{-\frac{1}{2\sigma^2}(y_i-\mu_i)^2}=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}\e{-\frac{1}{2\sigma^2}\sumin (y_i-\mu_i)^2}= \\ &=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}\e{-\frac{1}{2\sigma^2}(\Y-\boldsymbol{\mu})^T(\Y-\boldsymbol{\mu})}=\frac{1}{(2\pi\sigma^2)^{\frac{n}{2}}}\e{-\frac{1}{2\sigma^2}(\Y-\X\beta)^T(\Y-\X\beta)},
 \end{align*}
kde $\mu_i=\beta_0+\sum_{j=1}^n \beta_j x_{ij}$ a~$\boldsymbol{\mu}=(\mu_1,...,\mu_n)^T=\X\beta$.
 
 Pro~pevné $\sigma^2$ je 
 $$\max_\beta L(\beta,\sigma^2)\quad\Leftrightarrow\quad\min_\beta\underbrace{(\Y-\X\beta)^T(\Y-\X\beta)}_{g(\betab)}$$
 je opět pomocí derivací, ukážeme algebraický přístup.
 
 \begin{theorem}
 	Uvažujme model \ref{the_chosen_one} a~nechť $\textbf{e}\sim\NN(\textbf{0},\sigma^2 I_n)$. Potom $\wbeta$ je MLE $\beta$ právě tehdy, když $\wbeta$ je řešením soustavy rovnic
 	$$ \X^T\X\beta=\X^T\Y\qquad\text{(soustava normálních rovnic)}.$$
 	Je-li matice $\X^T\X$ singulární, má tato soustava jednoznačné řešení ve~tvaru 
 	$$ \wbeta=(\X^T\X)^{-1}\X^T\Y.$$
 	\begin{proof}
 		\begin{enumerate}[$\Leftarrow$]
 			\item Ukážeme, že každé řešení $\wbeta$ soustavy $\X^T\X\beta=\X^T\Y$ minimalizuje $g(\betab)$ a~pro~každé $\betab$ platí, že 
 			$$ g(\bbeta)=(\Y-\X\textbf{eta}^T(\Y-\X\beta))=\Y^T\Y-2\underbrace{\Y^T\X\beta}_{\wbeta^T\X^T\X}+beta^T\X\X\beta=\Y^T\Y-2\wbeta^T\X^T\X\beta+\beta^T\X^T\X\beta$$
 			má platit i~pro~$\wbeta$:
 			$$ g(\wbeta)=\Y^T\Y-2\wbeta^T\X^T\X\wbeta+\wbeta^T\X^T\X\wbeta=\Y^T\Y-\wbeta^T\X^T\X\wbeta$$
 			a tedy 
 			\begin{align}
 			g(\bbeta)-g(\wbeta)&=\beta^T\X^T\X\beta-2\wbeta^T\X^T\X\beta+\wbeta\X^T\X\wbeta=(\X\beta-\X\wbeta)^T(\X\beta-\X\wbeta)=\\&\label{tohle}=\big( \X(\beta-\wbeta) \big)^T\big( \X(\beta-\wbeta) \big)=\langle \X(\beta-\wbeta),\X(\beta-\wbeta) \rangle\geq 0,\quad\forall\beta,
 			\end{align}
 			 tedy $\wbeta$ minimalizuje $g(\bbeta)$ a~je tedy MLE parametru $\beta$.
 		\end{enumerate}
 	\begin{enumerate}[$\Rightarrow$]
 		\item Předpokládejme, že $\wbeta_1$ minimalizuje $g(\beta)$ (je tedy MLE). To potom znamená, že $g(\wbeta_1)\leq g(\beta),~\forall \beta$, speciálně $g(\wbeta_1)\leq g(\wbeta)$, kde $\wbeta$ je řešení soustavy $\X^T\X\beta=\X^T\Y$. Z~rovnice \ref{tohle} vyplývá, že $g(\wbeta_1)\geq g(\wbeta)$. Celkem tedy $g(\wbeta_1)=g(\wbeta)$. Dosazením do~\ref{tohle} dostaneme, že 
 		$$ 0=g(\wbeta_1)-g(\wbeta)=\langle \X(\wbeta_1-\wbeta),\X(\wbeta_1-\wbeta) \rangle$$
 		a tedy $\X(\wbeta_1-\wbeta)=\textbf{0}$ Potom ale vynásobením $\X^T$ zleva dostaneme, že 
 		$$ \X^T\X\wbeta_1\underbrace{\X^T\X\wbeta}_{\X^T\Y}=0\quad\Rightarrow\quad \X^T\X\wbeta_1=\X^T\Y$$
 		 a~$\wbeta_1$ splňuje soustavu $\X^T\X\beta=\X^T\Y$.
 		 
 		 Aby byl důkaz korektní, je třeba ukázat, že soustava $\X^T\X\beta=\X^T\Y$ má vždy alespoň 1 řešení. Pokud existuje $(\X^T\X)^{-1}$, není co dokazovat, řešení máme přímo. Co když je ale $\X^T\X$ singulární?
 	\end{enumerate}
 	\end{proof}
 \end{theorem}
\begin{lemma}
	Soustava lineárních rovnic $\mathbb{A}\X=\Y$ má řešení právě tehdy, když $\langle \Y,\textbf{z}\rangle=0$ pro~všechna $\textbf{z}$ splňující $\mathbb{A}\textbf{z}=\textbf{0}$.
\end{lemma}
\begin{theorem}
	Soustava normálních rovnic $\X^T\X\beta=\X^T\Y$ má vždy alespoň jedno řešení.
	\begin{proof}
		Musíme ukázat, že $\langle \X^T\Y,\textbf{z}\rangle=0,~\forall\textbf{z}$ splňující $\X^T\X\textbf{z}=\textbf{0}$. Potom
		$ \X^T\X\textbf{z}=\textbf{0}~\Rightarrow~ \langle \X^T\X\textbf{z},\textbf{z}\rangle=\langle \X\textbf{z},\X\textbf{z}\rangle=0$ a~tedy $\X\textbf{z}=\textbf{0}$. Celkem tedy $\langle\X^T\Y,\textbf{z}\rangle=\langle\Y,\X\textbf{z}\rangle=0$. Obecně totiž platí, že $\langle \X,\mathbb{A}\Y\rangle=\langle \mathbb{A}^T\X,\Y\rangle$.
	\end{proof}
\end{theorem}
\begin{remark}
	Z vět vyplývá, že MLE $\beta$ může být nalezeno řešením $m+1$ lineárních rovnic o~$m+1$ neznámých. Málokdy existuje analytické řešení, je třeba použít numerické metody. Matice $\X^T\X$ může být v~praktických aplikacích špatně podmíněná, což ovlivňuje numerickou přesnost $\wbeta$. Proto se~často užívají metody jako Choleského rozklad, QR rozklad, singulární rozklad (SVD).
	
	Odvodili jsme to pro~normální chyby. Minimalizace $g(\betab)$ lze ale použít i~pro~jiné druhy chyb, potom se~$\wbeta$ nazývá \textbf{ordinary least squares estimate (OLS)} (obyčejné nejmenší čtverce). Asi nejužívanější metoda  pro~oblast $\beta$. 
	
	Jak poznat, že mají normální rovnice jednoznačné řešení bez~nutnosti výpočtu $\X^T\X$?
\end{remark}
\begin{theorem}
	Matice $\X^T\X$ je nesignulární právě tehdy, když jsou sloupce matice $\X$ LN.
	\begin{proof}
		\begin{enumerate}[$\Leftarrow$]
			\item Sporem. Nechť jsou sloupce $\X$ LN a~matice $\X^T\X$ singulární, tzn. $\exists c\neq0$ tak, že $\X^T\X c=0$. Potom
			$$ 0=\langle c,\X^T\X c\rangle=\langle \X c,\X c\rangle\quad \Rightarrow\quad\X c=0,\qquad\sum c_i\X_i^c=0,$$
			kde $c=(c_1,...,c_m)^T$ a~$\X_i^c$ je $i$-tý sloupec matice $\X$. Potom sloupce $\X$ jsou LZ. Spor.
		\end{enumerate}
	\begin{enumerate}[$\Rightarrow$]
	\item Sporem. Předpokládejme, že $\X^T\X$ je regulární a~sloupce $\X$ LZ. Potom existuje $c\neq0$ takové, že $\X c=0$, $\X^T\X c=0$. Z~toho vyplývá, že $\X^T\X$ je singulární. Spor.
\end{enumerate}
	\end{proof}
\end{theorem}
\begin{remark}
	Pokud $\X_{nx(m-1)},~n>m+1,~h(\X)=m+1,~\textbf{e}\sin\NN(0,\sigma^2 I_m)$. Potom existuje jednoznačné řešení normálních rovnic $\wbeta=(\X^T\X)^{-1}\X^T\Y$.
\end{remark}
\begin{remark}
	Pokud jsou sloupce $\X$ LZ, je $\X^T\X$ sigulární, což je většinou detekováno numerickou metodou výpočtu $\wbeta$.
\end{remark}
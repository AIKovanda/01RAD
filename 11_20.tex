\chapter{Pages 11-20}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\hbn}{\widehat{\beta}_0}
\newcommand{\hbj}{\widehat{\beta}_1}
\newcommand{\hbi}{\widehat{\beta}_i}
\newcommand{\lxn}{\overline{x}_n}
\newcommand{\hxn}{\overline{x}_n}
\newcommand{\hYn}{\widehat{Y}_n}
\newcommand{\hYi}{\widehat{Y}_i}
\newcommand{\sxx}{s_{xx}}

\begin{theorem}
	Za předpokladu předchozí věty platí
	\begin{equation*}
		\E (s_n^2) = \sigma^2
	\end{equation*}
	($s_n^2$ je nestranný odhad $\sigma^2$).
\end{theorem}


\begin{proof}
	\begin{equation*}
		\E(s_n^2)  = \frac{1}{n-2} \E \sum_{i=1}^{n} (Y_i - \hYi)^2 = \frac{1}{n-2} \underbrace{\sumin \E (Y_i - \hYi)^2}_{\text{ozn. } A}
	\end{equation*}
	
	Protože $\E(\hYi)  = \E(\hbn + \hbj x_i) = \beta_0 + \beta_i x_i = \E Y_i$, platí, že:
	\begin{equation*}
	\E(Y_i - \hYi)^2   = \Var(Y_i - \hYi) = \E (Y_i - \hYi)^2 - \underbrace{(\E(Y_i - \hYi)^2)}_{= 0}
	\end{equation*}
	
	Dostáváme tak	
	\begin{align*}
		A & = \sum_{i=1}^{n} \Var(Y_i - \hYi) = \sum_{i=1}^n [ \Var(Y_i) + \Var(\hYi) - 2 \Cov(Y_i, \hYi) ] =  \\
		& = n \sigma^2 + \sum_{i=1}^n \Var(\hYi) - 2 \sum_{i=1}^n \Cov(Y_i, \hYi) \tag{$\#$}
	\end{align*}
	
	Rozepíšeme
	\begin{equation*}
		\Var \hYi  = \Var(\hat{\beta}_o + \hbj x_i) = \Var \hbn + x_i^2 \Var \hbj + 2 x_i,
	\end{equation*}
	kde
	\begin{equation*}
		\Cov(\hbn, \hbj) = \Cov(\hYn - \hbj \hxn, \hbj) = \underbrace{\Cov(\hYn, \hbj)}_{= 0 \text{ (viz. dříve)}} - \hxn \underbrace{\Var(\hbj)}_{\frac{\sigma^2}{\sxx}} = -\frac{\sigma^2 \hxn}{\sxx}
	\end{equation*}
	a tedy
	\begin{align*}
		\Var \hYi & = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}_n^2}{\sxx} + x_i^2 \frac{1}{\sxx} - \frac{2 x_i \bar{x}_n}{\sxx} \right] = \sigma^2 \left[ \frac{1}{n} + \frac{(x_i - \bar{x}_n)^2}{\sxx}\right] \\
		\sum_{i=1}^n \Var \hYi & = \sigma^2 + \frac{\sigma^2}{\sxx} \underbrace{\sum_{i=1}^n (x_i - \bar{x}_n)^2}_{ = \sxx} = 2\sigma^2
	\end{align*}
	
	Následně máme
	\begin{align*}
	\Cov(Y_i, \hYi) & = \Cov(Y_i, \hbn + \hbj x_0) = \Cov(Y_i, \hbn) + x_i \Cov(Y_i, \hbj) \\
	\Cov(Y_i, \hbj) & = \frac{1}{\sxx} \sum_{j=1}^{n} (x_j - \bar{x}_n) \underbrace{\Cov(Y_i, Y_j)}_{= 0 \text{ pro } i \neq j} = \frac{\sigma^2(x_i - \bar{x}_n)}{\sxx} \\
	\Cov(Y_i, \hbn) & = \Cov(Y_i, \bar{Y}_n - \bar{x}_n \hbj) = \Cov(Y_i, \bar{Y}) - \bar{x}_n \Cov(Y_i, \hbj) = \frac{\sigma^2}{n} - \frac{\bar{x}_n \sigma^2 (x_i - \bar{x}_n)}{\sxx} \\
	\end{align*}
	a tedy
	\begin{align*}
		\Cov(Y_i, \hYi) & = \frac{\sigma^2}{n} - \frac{\bar{x}_n \sigma^2 (x_i - \bar{x}_n)}{\sxx} + \frac{x_i \sigma^2 (x_i - \bar{x}_n)}{\sxx} = \frac{\sigma^2}{n} + \frac{\sigma^2}{\sxx}(x_i - \bar{x}_n)^2 \\
		\sum_{i=1}^n \Cov(Y_i, \hYi) & = \sigma^2 + \frac{\sigma^2}{\sxx} \sum_{i=1}^n (x_i - \bar{x}_n)^2 = 2 \sigma^2 \\
	\end{align*}
	Dosazením do ($\#$) dostaneme
	\begin{equation*}
		A  = n\sigma^2 + 2\sigma^2 - 4\sigma^2
	\end{equation*}
	a celkem máme
	\begin{equation*}
		\E(s_n^2)  = \frac{1}{n-2} A = \sigma^2.
	\end{equation*}
\end{proof}

\begin{corollary}
	Nechť platí předpoklady věty 1 a nechť $e_1, \dots, e_n$ i.i.d. $N(0,\sigma^2)$. Potom platí:
	\begin{enumerate}[a)]
		\item $\frac{(n-2)s_n^2}{\sigma^2} \sim \chi(n-2)$
		\item $s_n^2$ je nezávislé na $\hbn$ a $\hbj$.
	\end{enumerate}
\end{corollary}

\begin{proof}
	Vyplyne z obecnějších tvrzení pro vícerozměrnou regresi.
\end{proof}

\begin{remark}
	Spočetli jsme
	\begin{equation*}
		\underbrace{\Var(\hbn)}_{\text{ozn. } \sigma^2(\hbn)} = \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}_n^2}{\sxx} \right] \quad \text{a} \quad \underbrace{\Var(\hbj)}_{\text{ozn. } \sigma^2(\hbj)} = \frac{\sigma^2}{\sxx}
	\end{equation*}
	
	Nestranné odhady jsou:
	\begin{align*}
		\sigma^2(\hbn) & = s_n^2 \sigma^2 \left[ \frac{1}{n} + \frac{\bar{x}_n^2}{\sxx} \right] = s_n^2 \delta_0 \\
		\sigma^2(\hbj) & = \frac{s_n^2}{\sxx} = s_n^2 \delta_1,
	\end{align*}
	kde $\delta_0$ a $\delta_1$ jsou tzv. variance multiplication factors.
	
	Odhady směrodatné odchylky veličin $\hbn$ a $\hbj$ pak jsou
	\begin{equation*}
		\hat{\sigma}(\hbn) = s_n \sqrt{\delta_0} \quad \text{a} \quad \hat{\sigma}(\hbj) = s_n \sqrt{\delta_1},
	\end{equation*}
	kterým se pak říká standardní chyby odhadů $\hbn$ a $\hbj$. Hrají zásadní roli při konstrukci IS a TH.
\end{remark}

\section{Gauss - Markov theorem}

\begin{itemize}
	\item Chyby normální $\implies$ LSE pro $\hbn, \hbj$ je MLE ... parametrů (eficientní odhad)
	\item Pokud nejsou chyby normální, jaké je opodstatnění použít LSE?
	
	Ukážeme, že LSE jsou BLUE (best linear unbiased estimators), tedy lineární nestranné odhady s minimálním rozptylem
	\item Je ale třeba poznamenat, že můžou existovat nelineární nebo vychýlené odhady parametrů $\beta_0, \beta_1$, které jsou eficientnější než LSE, pokud se rozdělení chyb liší výrazně od normálního (tím se zabývá robustní regresní analýza).
\end{itemize}

Uvažujme model
\begin{equation*}
	Y_i = \beta_0 + \beta_1 x_i + e_i, \quad i = 1, \dots, n \text{ (*)}
\end{equation*}

\begin{define}
	Lineární odhad parametru $\beta$ je statistika tvaru
	\begin{equation*}
		\hat{\beta} = \sumin c_i Y_i,
	\end{equation*}
	kde $c_i$ jsou dané reálné konstanty a $i = 1, \dots, n$.
\end{define}

\begin{theorem}
	Nechť $e_1, \dots, e_n$ v modelu (*) jsou nekorelované a mají stejný rozptyl $\Var(e_i) = \sigma^2, i = 1, \dots, n$. Potom LSE $\hat{\beta}_j, j = 0,1$ je BLUE parametru $\beta_j$.
\end{theorem}

\newcommand{\pd}{\delta}

\begin{proof}
	Ukážeme pro $\beta_1$, pro $\beta_0$ je důkaz podobný.
	
	Nechť $\hbj = \sumin c_i Y_i$, pak
	
	$\Var \hbj = \sumin c_i^2 \Var Y_i = \sigma^2 \sumin c_i^2$
	
	Aby byl $\hbj$ nestranný, musí platit $\E \hbj = \beta_1$, tedy
	
	$\E \hbj = \sumin c_i \E Y_i = \beta_0 \sumin c_i + \beta_1 \sumin c_i x_i \overset{!}{=} \beta_1$
	
	protože to musí platit pro lib. $\beta_0, \beta_1$, dostáváme
	\begin{equation*}
		\sumin c_i = 0 \quad \text{a} \quad \sumin c_i x_i = 1.
	\end{equation*}
	
	Hledání lineárního, nestranného odhadu $\beta_1$ je tedy redukováno na minimalizaci $\sumin c_i^2$ za vazebných podmínek $\sumin c_i = 0 \quad \text{a} \quad \sumin c_i x_i = 1$.
	
	Lagrangeova funkce: $L = \sumin c_i^2 - 2 \lambda_1 (\sumin c_i) - 2 \lambda_2 (\sumin c_i x_i - 1)$.
	
	\begin{gather*}
		\frac{\partial L}{\partial c_i} = 2 c_i - 2 \lambda_1 - 2 \lambda_2 x_i = 0, \quad i = 1, \dots, n \\
		\frac{\partial L}{\partial \lambda_1} = -2 (\sumin c_i) = 0 \\
		\frac{\partial L}{\partial \lambda_2} = -2 (\sumin c_i x_i - 1) = 0
	\end{gather*}
	
	Sečteme prvních $n$ rovnic
	\begin{equation*}
		\underbrace{\sumin c_i}_{= 0} - n \lambda_1 - \lambda_2 \sumin x_i = 0 \implies n\lambda_1 + \lambda_2 \sumin x_i = 0 \implies \lambda_1 = - \lambda_2 \lxn
	\end{equation*}
	
	Sečteme dále prvních $n$ rovnic vynásobených $x_i$:
	\begin{gather*}
		\sumin c_i x_i - \lambda_1 \sumin x_i - \lambda_2 \sumin x_i^2 = 0 \\
		\implies \lambda_1 \sumin x_i + \lambda_2 \sumin x_i^2 = 1 \\
		 -\lambda_2 \lxn \cdot n \lxn + \lambda_2 \sumin x_i^2 = 1 \\
		 \lambda_2 \left( \sumin x_i^2 - n \lxn^2 \right) = 1 \implies \lambda_2 = \frac{1}{\sxx} \quad \text{a} \quad \lambda_1 = - \frac{\lxn}{\sxx}
	\end{gather*}
	
	Dosadíme za $\lambda_1, \lambda_2$:
	\begin{equation*}
		c_i + \frac{\lxn}{\sxx} - \frac{x_i}{\sxx} = 0 \implies c_i = \frac{x_i - \lxn}{\sxx}
	\end{equation*}
	a $\hbj = \frac{1}{\sxx} \sumin (x_i - \lxn) Y_i$, což je LSE.

\end{proof}

\begin{remark}
	Ukázali jsme pouze, že to je stacionární bod, že je tam i minimum ukážeme v obecnější větě ve vícerozměrné regresi.
\end{remark}

\section{IS pro $\beta_0, \beta_1$}

\begin{itemize}
	\item IS poskytují jistou "míru přesnosti" bodových odhadů
	\item pro jejich konstrukci potřebujeme znát rozdělení pravděpodobnosti bodového odhadu
	\item budeme tedy uvažovat normalitu chyb
	\item spočtené IS se ale často používají, i když rozdělení chyb není normální, jejich použití se zdůvodňuje tím, že LSE odhady par. $\beta$ jsou lineární funkcí $Y_i, i = 1, \dots, n$, což umožňuje aplikovat CLT a dostat asymptotickou normalitu odhadů $\hbn, \hbj$
\end{itemize}

Uvažujme model $Y_i = \beta_0 + \beta_1 x_i + e_i$, $e_i$ i.i.d $N(0,\sigma^2)$. Víme:
\begin{equation*}
	\hat{\beta}_i \sim N(\beta_i, \sigma^2(\hbi)), \quad \frac{(n-2)s_n^2}{\sigma^2} \sim \chi^2(n-1) \text{\; a nezávisí na \;} \hbn, \hbj.
\end{equation*}

\begin{remark}
	\begin{equation*}
		X \sim N(0,1), Y \sim \chi^2(n), X, Y \text{ nezávislé } \implies \frac{X}{\sqrt{Y/n}} \sim t(n)
	\end{equation*}
\end{remark}

Tedy
\begin{equation*}
	T_i = \frac{\frac{\hbi - \beta_i}{\sigma(\hbi)}}{\frac{s_n}{\sigma}} = \frac{\hbi - \beta_i}{\hat{\sigma}}(\hbi) \sim t(n-2, i = 0,1)
\end{equation*}
neboť $\sigma(\hbi) = \sigma \sqrt{\delta_i}$ a $\hat{\sigma}(\hbi) = s_n \sqrt{\delta_i}$.

Tzn. $P \left[ -t_{1-\alpha/2}(n-2) \leq \frac{\hbi - \beta_i}{\hat{\sigma}}(\hbi) \leq t_{1-\alpha/2}(n-2) \right]$ a vyjádřením $\beta_i$ dostaneme
\begin{equation*}
	P \left[ \hbi - t_{1-\alpha/2}(n-2) \hat{\sigma}(\hbi) \leq \beta_i \leq  \hbi + t_{1-\alpha/2}(n-2) \hat{\sigma}(\hbi) \right] = 1 - \alpha
\end{equation*}
a tedy $(\hbi \pm t_{1-\alpha/2}(n-2) \hat{\sigma}(\hbi))$ je $100(1-\alpha)\%$ IS pro $\beta_i, i = 0,1$.

Dosazením za $\hat{\sigma}(\hbi)$ dostaneme

\begin{itemize}
	\item $100(1-\alpha)\%$ IS pro $\beta_0$: $\hbn \pm t_{1-\alpha/2}(n-2) \cdot s_n \sqrt{\frac{1}{n} + \frac{\lxn^2}{\sxx}}$
	\item $100(1-\alpha)\%$ IS pro $\beta_1$: $\hbj \pm t_{1-\alpha/2}(n-2) \cdot s_n \frac{1}{\sqrt{\sxx}}$
\end{itemize}

\begin{remark}
	Z tvarů IS lze pozorovat, že IS pro $\beta_0$ bude ve většině praktických případů širší než IS pro $\beta_1$, tzn. směrnice je obecně odhadnuta s větší přesností než absolutní člen (intercept).
\end{remark}


\begin{remark}
	Někdy se konstruují simultánní IS pro oba parametry. (Obr?) Zmíníme podrobněji u vícerozměrné regrese.
\end{remark}


\section{TH pro $\beta_0, \beta_1$}

Chtěli bychom ověřit platnost předpokladu lineárního vztahu mezi $x$ a $y$.

Předpokládejme nyní, že model je lineární a že $x$ je jediná dostupná vysvětlující proměnná. Otázoku je, zda je $x$ užitečná ve vysvětlení variability v $y$, chceme tedy rozhodnout mezi dvěma modely:
\begin{equation*}
	Y_i = \beta_0 + e_i \quad \text{a} \quad Y_i = \beta_0 + \beta_1 x_i + e_i
\end{equation*}
tzn. otestovat hypotézu $\hypothesis{\beta_1 = 0}{}\beta_1 \neq 0$.

Pokud nezamítneme $H_0$, závěr bude, že $x$ nevysvětluje nic z variability $y$ a není v modelu významné. Pokud zamítneme $H_0$, znamená to, že $x$ je významné.

\begin{remark}
	Tyto závěry jsou správné pouze za předpokladu, že model je lineární!
	\begin{itemize}
		\item nezamítnutí $H_0$ nemusí znamenat, že $x$ není užitečná, může to pouze indikovat, že vztah mezi $y$ a $x$ není lineární
		\item zamítnutí $H_0$ naopak ří, že existuje lineární trend mezi $x$ a $y$, ale mohou tam být i jiné typy závislosti
	\end{itemize}
\end{remark}

Pro konstrukci testů využijeme odvozené IS.

\begin{remark}
	Opakování: $\hypothesis{\theta = \theta_0}{\theta \neq \theta_0} \implies (\underline{\theta}, \bar{\theta})$ je $100(1-\alpha)\%$ IS pro $\theta$. Pak $W = \{ x | \theta_0 \notin (\underline{\theta}, \bar{\theta}) \}$ je kritický obor test na hladině $\alpha$.
\end{remark}

$H_0: \beta_1 = 0$ zamítneme, pokud $0 \notin \left( \hbj \pm t_{1-\alpha/2}(n-2) \cdot  \frac{s_n}{\sqrt{\sxx}} \right)$, tzn.
\begin{gather*}
	\text{buď } \hbj + t_{1-\alpha/2}(n-2) \cdot  \frac{s_n}{\sqrt{\sxx}} < 0 \iff  \hbj \frac{\sqrt{\sxx}}{s_n} < - t_{1-\alpha/2}(n-2) \\
	\text{nebo } \hbj - t_{1-\alpha/2}(n-2) \cdot  \frac{s_n}{\sqrt{\sxx}} > 0 \iff \hbj \frac{\sqrt{\sxx}}{s_n} > t_{1-\alpha/2}(n-2)
\end{gather*}
A zapsáno dohromady
\begin{equation*}
	|T_n| = |\hbj| \frac{\sqrt{\sxx}}{s_n} > t_{1-\alpha/2}(n-2).
\end{equation*}

\begin{remark}
	Intuitivní interpretace: $|T_n| = |\hbj| \frac{\sqrt{\sxx}}{s_n} = \frac{|\hbj|}{\hat{\sigma}(\hbj)}$ je převrácená hodnota relativní chyby.
	
	Pokud je $\beta_1$ dobře odhadnuto, očekáváme malý rozptyl $\hat{\sigma}(\hbj)$, tedy $T$ bude velké.
	
	t-test tedy říká, že zamítneme $H_0$, pokud je relativní chyba odhadu malá.
\end{remark}
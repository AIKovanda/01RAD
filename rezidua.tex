\chapter{Rezidua, diagnostika a~transformace}
\begin{itemize}
 \item Je třeba ověřit adekvátnost modelu. Máme $\RMR^2, \mathrm{t}, \text{F}$ statistiky, ty ale byly odvozeny za~předpokladu linearity modelu a~dalších podmínek na~náhodné chyby. Pro~ověření je důležitý nástroj analýza reziduí
 \item Je také třeba ověřit vliv jednotlivých pozorování na~model -- analýza odlehlých (outliers) a~influenčních pozorování. (Velké reziduum pro~$i~$-té pozorování naznačuje problém s~modelem, ale může to být i~naopak, vlivné pozorování nemusí mít velké reziduum.)
 \item Pokud detekujeme nějaké problémy s~modelem, mohou pomoci transformace proměnných nebo metoda na~korekci nekonstantního rozptylu.
\end{itemize}
\section{Rezidua}
Připomenutí: víme už, že platí následující vztahy:
 $$
 \hyb = \X \wbetab = \Hm \y, \quad \text{kde} \quad \Hm = \X (\X^T \X)^{-1} \X^T,
 $$
 $$
 \heb = \y - \hyb = (\In - \Hm) \y = (\In - \Hm) \eb.
 $$
Dále jsme ukázali
 $$
\E [\heb] = \nula \quad \text{a} \quad \Cov(\heb) = \sigma^2 (\In - \Hm).
 $$
Pokud navíc
$\eb \sim \NN_n (\nula,\sigma^2 \In)$ potom $\heb \sim \NN_n \left(\nula,\sigma^2 (\In - \Hm)\right)$. Když označíme $h_{ii} = \Hm_{ii} $, pak $ \he_i \sim \NN_n (\nula, \sigma^2 (1 - h_{ii}) $ a $ \Cov(\he_i, \he_j) = -\sigma^2 h_{ij} $.

Obecně bývá vhodnější pracovat se~standardizovanými rezidui, protože $\D [\he_i] = \sigma^2 (1 - h_{ii})$, ale pro~$r_i = \frac{\he_i}{\sigma \sqrt{1-h_{ii}}}$ platí $\D [r_i] = 1$.
 Parametr $\sigma$ odhadneme pomocí $s_n = \sqrt{\frac{1}{n-m-1} \SSE}$, čímž dostaneme
 $$
  \widehat{r}_i = \frac{\he_i}{s_n \sqrt{1 - h_{ii}}}, \quad \text{kde }  i\in\widehat{n},
 $$
kterým se říká interně studentizovaná rezidua (někdy také standardizovaná rezidua). V R se jedná o funkci \verb|rstandard()|.

Pokud $\sigma^2$ odhadneme na~základě modelu, ve~kterém bylo vynecháno $i$-té pozorování, označíme tento odhad $\wsigma^2_{(-i)}$, potom
 $$
 \widehat{t}_i = \frac{\he_i}{  \wsigma^2_{(-i)} \sqrt{1 - h_{ii}}}, \quad \text{kde } i\in\widehat{n}.
 $$
 Říká se jim externě studentizovaná rezidua (někdy také studentizovaná rezidua). V R se jedná o funkci \verb|rstudent()|.
 
Například $\wsigma^2_{(-i)} = \frac{1}{n-m-2}\SSE_{(-i)}$ je nestranný odhad $\sigma^2$ v~modelu bez $i$-tého pozorování.

\begin{remark}
Platí:
\begin{itemize}
\item Pokud je $h_{ii}$ malé, pro~velké $n$ by se~měly $\widehat{\text{e}}_i, \widehat{\text{r}}_i, \widehat{\mathrm{t}}_i$ chovat přibližně stejně a~$\widehat{\text{r}}_i,\widehat{\mathrm{t}}_i \approx \NN (0, 1)$.
\item Pro~malé $n$ $(n < 20)$ a/nebo $h_{ii} \approx 1$ je preferováno použít $\widehat{\text{r}}_i$ nebo $\widehat{\mathrm{t}}_i$. Aktuálně bývá častěji doporučována $\widehat{\mathrm{t}}_i$ ($i$-té pozorování s~velkými $h_{ii}$ může zvyšovat odhad $\sigma^2$ a~tím snižuje velikost svého rezidua).
\item V anglické literatuře se označuje $h_{ii}$ jako \textbf{leverage} -- potenciál $i$-tého pozorování (leverage point = píkový bod / vzdálený bod). $h_{ii}$ hraje zásadní roli v~diagnostice modelu, proto teď probereme jeho základní vlastnosti.
\end{itemize}
\end{remark}

\subsection{Vlastnosti potenciálu $\hii$}

\begin{itemize}
\item $\D [\he_i] = \sigma^2 (1 - h_{ii}) \geq 0 \Rightarrow h_{ii} \leq 1$.
\item $\Hm^2 = \Hm \Rightarrow h_{ii} = \sumjn h_{ij} h_{ji} = \sumjn (h_{ij})^2$, tedy $h_{ii} > 0$. Dá se~ukázat i silnější tvrzení: $h_{ii} \geq \frac{1}{n}$.
\item $\Hm\X = \X \Rightarrow \sumjn h_{ij} x_{j1} = \sumjn h_{ij} = x_{i1} = 1$ tedy $\sumjn h_{ij} = 1 \quad \forall j \in \widehat{n}$ (v modelu s interceptem).
\item Význam $h_{ii}$ vyplyne z~následujících úvahy:
 $$
 \hyb = \Hm \y \Rightarrow \widehat{y}_i = \sumjn h_{ij}y_j = h_{ii} y_i + \sum\limits_{\substack{j = 1 \\ i\neq j}}^n h_{ij} y_j.
 $$
 \begin{itemize}
 	\item Pokud $h_{ii} \approx 1$, pak $\widehat{y}_i \approx y_i$ a~model je nucen proložit přímku bodem ($\x_i, y_i$), i~když když tam neplatí.
 	\item Body s~\uv{velkým $h_{ii}$} -- body s~velkým potenciálem (high leverage points). Tyto body by měly být detekovány pro~další zkoumání.
 \end{itemize}

 \item Otázka je, jaká hodnota $h_{ii}$ je \uv{velká}.
\end{itemize}

\subsubsection{Heuristické pravidlo}
Platí $\sumin h_{ii} = \text{tr}(\Hm) = m + 1$, tzn. $\frac{m+1}{n}$ je průměrná hodnota $h_{ii}$. Řekneme, že $i$-té pozorování má velký potenciál jestliže $h_{ii} > \frac{3(m+1)}{n}$ (stejně postupuje i~jazyk R).

\section{Grafy reziduí}
\begin{enumerate}[A)]
\item \textbf{Ověření normality} -- histogramy, Q-Q plots

Tyto obrázky nezávisí na~počtu nezávislých proměnných $x$, vše stejné jako v~jednoduché LR. Můžeme použít testy normality jako např. Shapiro-Wilk, Anderson-Darling a další.
\item Pro~\textbf{ověření funkční formy} pro~$\E [Y_x]$ a/nebo konstantního rozptylu se~nejčastěji používají:
\begin{enumerate}[1)]
\item grafy $\he_i, \widehat{r}_i$ nebo $\widehat{t}_i$ oproti~$\x_j^c$, $j = 1,\dots, m$, kde $\x_j^c$ je $j$-tý sloupec $\X$,
\item grafy $\he_i, \widehat{r}_i$ nebo $\widehat{t}_i$ oproti~$\widehat{y}_i$,
\item partial residual plots.
\end{enumerate}
Mezi testy konstantního rozptylu patří např. Breuch-Pagan nebo Levene test.
\end{enumerate}

\begin{remark}
Zdůvodnění:
\begin{enumerate}
\item Normální rovnice $\X^T (\y - \X \wbetab) = 0$ implikují $\X^T (\y - \hyb) = \X^T \heb = 0$.
 $$
 \text{Připomenutí:} \quad \text{Y}_i = \beta_1 x_i + e_i, \quad \wbeta_1 = \frac{\sumin x_i y_i}{\sumin x_i^2} = \frac{\x^T\y}{\norm{\x}^2 }
 $$
Pokud tedy naladíme LR model bez~interceptu pro~$\heb$ v~závislosti na~$\x_j^c$, odhad směrnice přímky bude
 $$
  \wbeta_j^* = \frac{(\x_j^c)^T \heb}{\norm{\x_j^c}^2} = 0.
 $$
Graf $\he_i, \widehat{r}_i, \widehat{t}_i$ oproti~$\x_j^c$ by měl dávat náhodně rozptýlené body kolem~osy $x$ (bez trendů, $\widehat{r}_i, \widehat{t}_i$ uvnitř $\approx \pm 2$).
Pokud tomu tak není, může to naznačovat nelinearitu v~$\x_j$ nebo nekonstantní rozptyl.
\item Ukázali jsme $\sumin \widehat{y}_i \he_i = 0$ pro~LM bez~interceptu. Pro~$\he_i$ oproti~$\widehat{y}_i$ tedy platí
 $$
  \wbetab = \frac{\heb^T \hyb}{| \hyb |^2} = \nula.
 $$
Body by opět měly být náhodně rozptýlené kolem~osy $x$. Případný trychtýřovitý tvar indikuje nekonstantní rozptyl, trendy pak indikují nelinearitu.
\end{enumerate}	
\end{remark}

\subsection{Partial residual plot}
\begin{itemize}
\item I~když grafy $\heb$ oproti~$\x_j^c$ a~$\hyb$ mohou indikovat nedostatky modelu, nemusí být zřejmé, jaké tyto nedostatky jsou.
\item V~SLR lze graf $\he_i$ oproti~$x_i$ použít pro~detekci nelinearity.
\item V~MLR mohou být tyto grafy (stejně jako scatterploty) zavádějící, protože $\heb$ závisí na~všech prediktorech, nemusí být tedy izolován efekt dané proměnné při~odstranění efektů ostatních.
\item Pro~zkoumané efekty $j$-té proměnné lze použít partial rezidual plots - lze je chápat jako jeho ekvivalent scatterplotu v~SLR.
\end{itemize}

\begin{define}
Definujme
$$
   \heb_j^* = \heb + \wbeta_j \x_j^c,
$$
 kde $\heb$ je vektor reziduí modelu, $\wbeta_j$ je LSE parametru $\beta_j$, $\x_j^c$ je $j$-tý sloupec $\X$.
\end{define}

\textbf{Partial residual plot} (PRP) je graf $\heb_j^*$ oproti~$\x_j^c$, $j = 1,\dots, m$. Pokud je model správný, měly by být body náhodně rozmístěné kolem~přímky se~směrnicí $\wbeta_j$.

\textbf{Zdůvodnění:} Vztah mezi~$\heb_j^*$ a~$\x_j^c$ má formu SLR bez~interceptu. Pokud je model správný, $\hei, i\in\widehat{n} $, splňují podmínku
$$\E \hei = 0 \quad \text{a} \quad \D \hei = \sigma^2(1 - \hii).$$
Má tedy smysl uvažovat regresní model pro~$\heb_j^*$ oproti~$\x_j^c$ ($\he_j^* = \gamma_j \x_j^c + \eb$).

\newcommand{\hg}{\widehat{\gamma}}

Pro odhad koeficientu platí, že
 $$
\hg_j = \frac{(\heb_j^* \x_j^c)}{\norm{\x_j^c}^2} = \frac{(\heb + \wbeta_j \x_j^c)^T \x_j^c}{\norm{\x_j^c}^2} = \frac{\heb^T \x_j^c + \wbeta_j \norm{\x_j^c}^2}{\norm{\x_j^c}^2} = \wbeta_j,
 $$
protože $\heb^T \x_j^c = 0$.

%(2 příklady - pdf 79-93 uprostřed str 6)

\newcommand{\wb}{\mathbf{w}}
\newcommand{\Xmj}{\X_{(-j)}}
\newcommand{\Xmi}{\X_{(-i)}}

\begin{remark}
	Partial residual ploty jsou někdy kritizovány za~nadhodnocování efektu $\x_j^c$. Alternativou mohou být \textbf{partial regression plots} (added variable plots).
\end{remark}

\subsection{Partial regression plot}
	\textbf{Motivace:} Ptáme se, zda přidat novou proměnnou do~modelu a~chtěli bychom odhadnout její efekt. Budeme tedy uvažovat rozšířený model
	 $$
	\Y = \X \betab + \gamma \wb + \eb,
	 $$
	kde $\wb$ je nový vektor regresorů. Model lze rozepsat jako
	 $$
	\Y = \begin{bmatrix}
	\X \wb
	\end{bmatrix} \begin{pmatrix}
	\betab \\ \gamma
	\end{pmatrix} + \eb = \X_w + \betab_w + \eb.
	 $$
	Použitím normálních rovnic pro~$\X_w$ lze odvodit formuli pro~$\hg$
	\begin{equation}
		\hg = \frac{\heb^T (\I - \Hm) \wb}{\norm{(\I - \Hm)\wb}^2},
		\label{Eq: hat gamma}
	\end{equation}
	kde $\hg$ je směrnice regresního modelu pro~$\heb$ v~závislosti na~$\wb_{res} = (\I - \Hm)\wb$ (tj. rezidua modelu pro~$\wb$ v~závislosti na~$\X$).
	
	Teď naopak uvažujme, že $\wb$ je sloupec původní $\X$, např. $\x_j^c$ a~ozn. $\Xmj$ matici $\X$ bez~sloupce $j$. V~předchozím modelu pomožme $\X = \Xmj$ a~$\wb = \x_j^c$. Potom  LSE $\wbeta_j$ parametru $\beta_j$ je
	 $$
	\wbeta_j = \frac{\heb_{(-j)}^T \x_{j, res}^c}{\norm{\x_{j,res}^c}^2},
	 $$
	kde $\heb_{(-j)}$ jsou rezidua modelu bez~$\x_j^c$, $\x_{j,res}^c = (\I - \Hm) \x_j^c$. Jedná se tedy o rezidua modelu pro~$\x_j^c$ v~závislosti na~ostatních proměnných, tedy $\Xmj$ (v $\x_{j,res}^c$ je odstraněn efekt ostatních regresorů).
	
	 $\wbeta_j$ je směrnice regresního modelu pro~$\heb_{(-j)}$ v~závislosti na~$\x_{j, res}$ \\ $\Rightarrow$ \textbf{added variable plot}: graf $\heb_{(-j)}$ proti~$\x_{j, res}, j = 1,\dots, m$. V R se jedná o funkci \verb|avPlots()| z knihovny \verb|car|.
	
	Pokud je model správný, body by měly být náhodně rozptýlené kolem~přímky se~směrnicí $\wbeta_j$ procházející počátkem. Pokud závislost na~$\x_j^c$ není lineární, projeví se~to odklonem bodů od~přímky.


\begin{remark}
Ze vztahu \eqref{Eq: hat gamma} je vidět, že MLR může být chápána jako posloupnost SLR, kde postupně vytváříme modely pro~novou proměnnou s~použitím reziduí modelu pro~předcházející proměnné.
\end{remark}

\section{PRESS rezidua (PRESS residuals, deleted residuals)}

Pokud budeme chtít model použít nejen k~vysvětlení vztahu mezi~proměnnými, ale také pro~predikci, hodila by se~míra vyjadřující jak dobře model predikuje (doposud jsme zkoumali jen jak dobře popisuje). Šlo by použít IS nebo IP, to bychom ale předem museli znát body, ve~kterých chceme predikovat.

Nejjednodušší přístup, jak měřit prediktivní přesnost modelu, by byl analýza reziduí pro~predikce hodnot v~nových bodech $\x$, obecně ale nemáme data $y$ v~těchto bodech. Jedna možnost je tak použít data, která máme k~dispozici.

\textbf{Postup}: Vynecháme jedno pozorování, naladíme model bez~tohoto pozorování a~porovnáme predikovanou a~pozorovanou hodnotu pro~vynechané pozorování.

\begin{define}
Předpokládáme, že vynecháme $i$-té pozorování. Ozn. $\wbetab_{(-i)}$ odhad $\betab$ v~modelu s~vynechaným $i$-tým pozorováním (M$_{(-i)}$) a~$\hy_{(-i)}$ predikovanou hodnotu modelem M$_{(-i)}$ v~bodě $\x_i^T$, tzn. $\hy_{(-i)} = \x_i^T \wbetab_{(-i)}$.

Potom
 $$
\he_{(-i)} = y_i - \hy_{(-i)}, \quad i\in\widehat{n}
 $$
nazýváme $i$-té \textbf{PRESS reziduum}.

 $\mathrm{PRESS} = \sumin \he_{(-i)}^2$ je užitečná míra přesnosti predikce.,
\end{define}

\begin{remark}
Otázka je, jak počítat $\he_{(-i)}, i\in\widehat{n} $. Pro~velké $n$ se~zdá, že to bude náročný problém, protože pro~každé $i \in \widehat{n}$ musíme naladit nový model. Naštěstí to není nutné, ukážeme totiž, že
$$
	\he_{(-i)} = \frac{\he_i}{1 - \hii},
$$
tzn. všechna $\he_{(-i)}$ lze snadno spočítat pomocí reziduí a~hodnot $\hii$ z~původního (plného) modelu.
\end{remark}

Zavedeme následující značení:
\begin{align*}
	\x_i^T & \text{ -- $i$-tý řádek matice }, \X \\
	\Xmi & \text{ -- matice $\X$ bez~$i$-tého řádku}.
\end{align*}

\newcommand{\xtx}{\left(\X^T \X \right)^{-1}}

\begin{theorem}
\label{Th: Rozklad_xtx}
Jestliže $\hii \neq 1$, potom
 $$
\left(\Xmi^T \Xmi \right)^{-1} = \xtx + \frac{\xtx \x_i \x_i^T \xtx}{1 - \hii},
 $$
kde $\hii$ je $i$-tý diagonální prvek matice $\Hm$.
\end{theorem}

\begin{proof}
Nejdříve ukážeme
\begin{align}
	& \X^T \X = \Xmi^T \Xmi + \x_i \x_i^T \label{Eq: plus} \\
	& \text{s rozměry } (m+1) \times (m+1) = (m+1) \times (m+1) + (m+1) \times (m+1). \notag
\end{align}

\newcommand{\sumkn}{\sum_{k = 1}^n}
\newcommand{\sumknn}{\sum_{k = 1}^{n-1}}

Kvůli značení předpokládáme $i = n$ (toho se~dá vždy dosáhnout permutací řádků $\X$). Potom
 $$
\left(\X^T \X \right)_{ij} = \sumkn x_{ki} x_{kj} = \sumknn x_{ki} x_{kj} + x_{ni} x_{nj}.
 $$
 $i,j$-tý prvek $\X_{(-n)}^T \X_{(-n)}$ je $\sumknn x_{ki} x_{kj}$ \\
 $i,j$-tý prvek $\x_n \x_n^T$ je $x_{ni} x_{nj}$, tzn. \eqref{Eq: plus} platí.

\begin{theorem}[Sherman-Morrison-Woodburry (z LA)]
Nechť $\Am$ je $n \times n$ invertibilní matice a~nechť $\z$ je $n \times 1$ sloupcový vektor. Jestliže $\z^T \inv{\Am} \z \neq 1$, potom matice $\Bm = \Am - \z^T \z$ je invertibilní a~platí
 \begin{equation}
\inv{\Bm} = \inv{\Am} + \frac{\inv{\Am} \z^T \z \inv{\Am}}{1 - \z^T \inv{\Am} \z}.
\label{Eq: SMW}
\end{equation}
\end{theorem}

Položme $\Am = \X^T \X$, $\z = \x_i$, $\Bm = \X_{(-i)}^T \X_{(-i)}$. Pak $\Bm = \Am - \z \z^T$, $\Am$ je invertibilní a
 $$
\z^T \inv{\Am} \z = \x_i^T \xtx \x_i = \left(\X \xtx \X^T \right)_{ii} = \hii \neq 1.
 $$
Užitím věty a dosazením do \eqref{Eq: SMW} dostaneme
 $$
\left(\X_{(-i)}^T \X_{(-i)} \right)^{-1} = \xtx + \frac{\xtx \x_i \x_i^T \xtx}{1 - \hii}.
 $$

\end{proof}

\begin{theorem}
\label{Th: ei}
	Nechť $\he_{(-i)}$ je $i$-té PRESS reziduum. Potom
	 $$
	\he_{(-i)} = \frac{\he_i}{1 - \hii}, \quad i\in\widehat{n}.
	 $$
\end{theorem}

\begin{proof}
	
	Nechť $\wbetab_{(-i)}$ je odhad $\betab$ v~modelu M$_{(-i)}$, tzn.
	 $$
	\wbetab_{(-i)} = \left(\X_{(-i)}^T \X_{(-i)} \right)^{-1} \X_{(-i)}^T \y_{(-i)},
	 $$
	kde $\y_{(-i)}$ je $\y$ bez~$i$-té složky $y_i$. To znamená, že
	\begin{align*}
	\hy_{(-i)} & = \x_i^T \wbetab_{(-i)} = \x_i^T \inv{\left(\X_{(-i)}^T \X_{(-i)} \right)} \X_{(-i)}^T \y_{(-i)} = \\
	 & = \left[\left(\X_{(-i)}^T \X_{(-i)} \right)^{-1} = \xtx + \frac{\xtx \x_i \x_i^T \xtx}{1 - \hii} \text{ viz věta \ref{Th: Rozklad_xtx}} \right] = \\
	 & = \x_i^T \xtx \X_{(-i)}^T \y_{(-i)} + \frac{1}{1 - \hii} \x_i^T \xtx \x_i \x_i^T \xtx \X_{(-i)}^T \y_{(-i)} = \\
	 & = S_1 + \frac{1}{1-\hii} S_2.
	\end{align*}
	
	Protože $\X_{(-i)}^T \y_{(-i)} = \X^T \y - y_i \x_i$, dostaneme
	\begin{align*}
	S_1 & = \x_i \xtx \left(\X^T \y - y_i \x_i \right) = \x_i^T \underbrace{\xtx \X^T \y}_{\wbetab} - y_i \underbrace{\x_i^T \xtx \x_i}_{\hii} = \\
	& = \x_i^T \wbetab - \hii y_i = \hy_i - \hii y_i.
	\end{align*}
	Podobně	
	 $$ S_2 = \underbrace{\x_i^T(\x^T\x)^{-1}\x_i}_{h_{ii}}\underbrace{\x_i^T(\x^T\x)^{-1}\x\y}_{\hyi} = y_i\underbrace{\x_i^T(\x^T\x)^{-1}\x_i}_{h_{ii}}\underbrace{\x_i^T(\x^T\x)^{-1}\x_i}_{h_{ii}} = h_{ii}\hyi-y_ih_{ii}^2, $$
	takže
	 $$ \hy_{(-i)} = \hyi-h_{ii}y_i+\frac{1}{1-h_{ii}}(h_{ii}\hyi-y_ih_{ii}^2). $$
	Celkem tedy
	\begin{align*}
	\he_{(-i)} & = y_i-\hy_{(-i)} = y_i(1+h_{ii})-\hyi-\frac{1}{1-h_{ii}}(h_{ii}\hyi-y_ih_{ii}^2) = \\
	& = \frac{1}{1-h_{ii}}\big(y_i(1-h_{ii}^2)-\hyi(1-h_{ii})-h_{ii}\hyi+y_ih_{ii}^2\big) = \frac{1}{1-h_{ii}}(y_i-\hyi) = \frac{\hei}{1-h_{ii}}.
	\end{align*}
\end{proof}

\renewcommand{\mini}[1]{#1_{(-i)}}
Budeme potřebovat podobné formule pro~$\wbetab-\wbetab_{(-i)}$ a~$\mini{\SSE}$.

\begin{theorem}
	\begin{enumerate}[1)]
		\item Nechť $\mini{\wbetab}$ značí $\LSE$ parametru $\betab$ v~modelu bez~$i$-tého pozorování. Potom platí
	 $$ \wbetab-\mini{\wbetab} = \frac{(\textbf{X}^T\textbf{X})^{-1}\x_i\hei}{1-\hii} = (\textbf{X}^T\textbf{X})^{-1}\x_i\mini{\he}. $$
	\item Pro~součet residuálních čtverců $\mini{\SSE}$ v~modelu bez~$i$-tého pozorování platí
	 $$ \mini{\SSE} = \sumjn \he_j^2-\frac{\he_i^2}{1-\hii}. $$
	\end{enumerate}

\begin{proof}
	\begin{enumerate}[1)]
		\item Stejně jako v~důkazu předchozí věty \ref{Th: ei} platí, že
		 $$ \mini{\wbetab} = S_1+\frac{1}{1-\hii}S_2, $$
		kde $S_1 = \wbetab-y_i\inv{(\textbf{X}^T\textbf{X})}\x_i$ a~$S_2 = \inv{\textbf{X}^T\textbf{X}}\x_i\hyi-y_i\core \x_i\hii$, tedy
		\[
		\begin{split}
		\wbetab-\mini{\wbetab}& = y_i\core\x_i-\frac{1}{1-\hii}\big(\core x_i\hyi-y_i\core \x_i\hii\big) = \\& = \core \x_i\Big(y_i-\frac{\hyi-y_i\hii}{1-\hii}\Big) = \core \x_i\Big(\frac{y_i-y_i\hii-\hyi+y_i\hii}{1-\hii}\Big) = \\& = \core \x_i\Big(\frac{y_i-\hyi}{1-\hii}\Big),
		\end{split}
		\]
		kde $\Big(\frac{y_i-\hyi}{1-\hii}\Big) = \frac{\hei}{1-\hii} = \mini{\he}$.
		\item \[
		\begin{split}
		\mini{\SSE}& = \big(\mini{\y}-\mini{\x}^T\mini{\wbetab}\big)^T\big(\mini{\y}-\mini{\x}^T\mini{\wbetab}\big) = \sum_{\substack{j = 1\\j\neq i}}^n(y_j-\x_j^T\mini{\wbetab})^2 = \\& = \sumjn (y_j-\x_j^T\mini{\wbetab})^2-(y_i-\x_i^T\mini{\wbetab})^2.
		\end{split}
		\]
		Z bodu 1) víme, že $\mini{\wbetab} = \wbetab-\frac{\core \x_i\hei}{1-\hii}$, tzn.
		 $$ \mini{\SSE} = \sumjn\Big(y_j-\x_j^T\wbetab+\frac{\x_j^T\core \x_i\hei}{1-\hii}\Big)^2-\Big(y_i-\x_i^T\wbetab+\frac{\x_i^T\core \x_i\hei}{1-\hii}\Big)^2. $$
		Protože $\x_j^T\core \x_i = h_{ij}$, dostaneme
		\[
	\begin{split}
	\mini{\SSE}& = \sumjn \Big(\he_j+\frac{h_{ij}\hei}{1-\hii}\Big)^2-\Big(\he_i+\frac{h_{ii}\hei}{1-\hii}\Big)^2 = \underbrace{\sumjn\Big(\he_j+\frac{h_{ij}\hei}{1-\hii}\Big)^2}_A-\frac{\hei^2}{(1-\hii)^2},\\
	A& = \sumjn \he_j^2+\frac{2\hei}{1-\hii}\underbrace{\sumjn h_{ij}\he_j}_0+\frac{\hei^2}{(1-\hii)^2}\underbrace{\sumjn h_{ij}^2}_{\hii}.
	\end{split}
	\]
		Protože pak $\hyb = \Hm\y$, tak $\Hm\hyb = \Hm^2\y = \Hm\y$, a~tedy $\Hm\heb = \Hm(\y-\hyb) = \Hm\y-\Hm\hyb = 0$, a~tedy
		 $$ \mini{\SSE} = \sumjn\he_j^2+\frac{\hei^2}{(1-\hii)^2}(\hii-1) = \sumjn\he_j^2-\frac{\hei^2}{1-\hii}. $$
	\end{enumerate}
\end{proof}
\end{theorem}

\begin{dusl}
	V modelu \eqref{the_chosen_one} s~$m+1$ parametry $\betab$ a~bez~$i$-tého pozorování platí, že $$ \EE{\mini{\SSE}} = (n-m-2)\sigma^2, $$
	takže
	 $$ \mini{\wsigma^2} = \frac{\mini{\SSE}}{n-m-2} $$ je nestranný odhad $\sigma^2$. Dále pak
	 $$ \mini{\wsigma^2} = \frac{(1-\hii)(n-m-1)s_n^2-\hei^2}{(1-\hii)(n-m-2)} = \frac{1}{n-m-2}\Big(\SSE -\frac{\hei^2}{1-\hii}\Big), $$ kde $s_n^2 = \frac{1}{n-m-1}\SSE$ (pro plný model).
	\begin{proof}
		Protože $\EE{\hei^2} = \D \hei = \sigma^2(1-\hii)$, dostaneme dle předchozí věty
		\[
		\begin{split}
		&\EE{\mini{\SSE}} = \sumjn\sigma^2(1-h_{jj})-\sigma^2 = \sigma^2\big[(n-1)-\underbrace{\sumjn h_{jj}}_{\trace(\Hm) = m+1}\big] = \sigma^2(n-m-2)\\
		&\mini{\wsigma^2} = \frac{1}{n-m-2}\mini{\SSE} = \frac{1}{n-m-2}\Big(\underbrace{\sumjn\he_j^2}_{\SSE = (n-m-1)s_n^2}-\frac{\hei^2}{1-\hii}\Big) = \frac{1}{n-m-2}\frac{(1-\hii)\SSE-\hei^2}{1-\hii}.
		\end{split}
		\]
	\end{proof}
\end{dusl}

\begin{remark}
	Dá se~ukázat, že $\mini{\SSE}$ a~$\hei$ jsou nezávislé náhodné veličiny. Protože $\frac{\mini{\SSE}}{\sigma^2}\sim\chi^2(n-m-2)$ a~$\frac{\hei}{\sigma\sqrt{1-\hii}}\sim\NN(0,1)$, dostaneme $\frac{\hei}{\mini{\wsigma}\sqrt{1-\hii}}\sim t(n-m-2)$.
\end{remark}

\begin{corollary}
	Uvažujme model \eqref{the_chosen_one}, kde $h(X) = m+1$ a~$\eb\sim\NN_m(0,\sigma^2 I_m)$. Nechť pro~$i\in\widehat{n}$ platí, že $\hii\neq1$. Potom $i$-té (externě) studentizované reziduum
	 $$ \widehat{t}_i\sim t(n-m-2). $$
\end{corollary}
\begin{remark}
	 $\widehat{t}_i$ lze použít pro~test hypotézy, zda je $i$-té pozorování odlehlé (outlier), tedy
	\[
	\begin{split}
	&H_0:~i\text{-té pozorování není odlehlé v~modelu }M\\
	&H_1:~i\text{-té pozorování je odlehlé v~}M,
	\end{split}
	\] kde odlehlé značí odlehlé vzhledem k~$M:~\Y\sim\NN_m(\textbf{X}\beta,\sigma^2 I_m)$ :\begin{enumerate}[a)]
		\item střední hodnota $i$-tého pozorování se~nerovná té dané modelem,
		\item pozorovaná hodnota $Y_i$ je neobvyklá za~platnosti $M$.
	\end{enumerate}
 $H_0$ zamítneme, pokud $$ |\widehat{t}_i|>t_{1-\frac{\alpha}{2}}(n-m-2)\approx u_{1-\frac{\alpha}{2}} \doteq 2\text{ pro~}\alpha = 0.05\text{ a~}n\text{ velká}. $$
Pokud test použijeme na~všechna pozorování, je potřeba aplikovat nějakou korekci na~vícenásobné testování, např. Bonferroni.
\end{remark}
\begin{remark}
	Vztah $\mini{\he}$ a~$\widehat{t}_i$ :
	 $$ \mini{\he} = \frac{\hei}{1-\hii}\quad\Rightarrow\quad\E\mini{\he} = 0\quad\wedge\quad \D \mini{\he} = \frac{\sigma^2}{1-\hii}. $$
	Standardizované PRESS reziduum $$ \frac{\mini{\he}}{\sqrt{D\mini{\he}}} = \frac{\frac{\hei}{1-\hii}}{\frac{\sigma}{\sqrt{1-\hii}}} = \frac{\hei}{\sigma\sqrt{1-\hii}} = r_i. $$
	Pokud použijeme $\mini{\wsigma^2}$ jako odhad $\sigma^2$, pak \textbf{studentizovaná PRESS rezidua} $$ \frac{\hei}{\mini{\widehat{{\sigma}}}\sqrt{1-\hii}} = \widehat{t}_i. $$
\end{remark}
\begin{remark}
	 $\mini{\he} = \frac{\hei}{1-\hii}$, a~proto, pokud $i$-té pozorování má velký potenciál $\hii$, bude $\mini{\he}$ mnohem větší, než $\hei$, pozorování s~velkým $\hii$ jsou dobře modelována, ale měřeno $\mini{\he}$ mohou špatně predikovat. To je další ukázka fit/prediction dilema.
	
	Stejný efekt nastává také pro~
	 $$ \wbetab_i-\mini{\wbetab} = \core\x_i\mini{\he}. $$
	Rozdíl může být \uv{malý}, pokud je \uv{fit} dobrý, ale může být také \uv{velký}, pokud je $\hii$ velké.
\end{remark}

\section{Míry influence}
I~pro~perfektní model mohou dva různé vzorky $(\x,\y)$ a~$(\x',\y')$ vést k~různým závěrům. Většinou máme k dispozici jen originální data, která nemusí být možné rozdělit na trénovací a validační/testovací. Bude nás proto zajímat, jaký vliv má $i$-té pozorování ($i$-tý řádek matice $\X$) na model.

Už víme, že velké $\hii$ indikuje, že $i$-té pozorování má velký vliv, a~velká rezidua naznačují možnou neadekvátnost modelu. Zavedeme míry, které budou oba dva faktory kombinovat. Využijeme k tomu přístup z~PRESS reziduí, tzn. budeme sledovat, jak velký vliv má vynechání $i$-tého pozorování na~$\wbetab$ a~$\hyb$.

\subsection{DFBETAS}
Vliv vynechání $i$-tého pozorování na~odhad $\wbetab$ měří rozdíl
 $$ \wbetab-\mini{\wbetab} = \core \x_i\frac{\hei}{1-\hii}, $$
 který bude základem pro naši analýzu.

\begin{description}
	\item[a) vliv $i$-tého pozorování na~$\wbeta_j$ :]
	 $$ \wbeta_j-\mini{\wbeta}{}_j = \frac{r_{ji}\hei}{1-\hii},\quad\text{kde } r_{ji} \text{ je }(j,i)\text{tý prvek matice }\Rm = \core\X^T. $$
	
	 $i$-té pozorování budeme považovat za~influenční na~$\beta_j$, pokud bude hodnota $\wbeta_j-\mini{\wbeta}{}_j$ velká. Protože $\wbeta_j$ je náhodná veličina, jestli jsou hodnoty \uv{velké} bychom měli měřit relativně vzhledem k~s.d.$(\wbeta_j)$, což je $\sigma\sqrt{v_j}$, $v_j = \core_{jj})$. Pokud ji odhadneme pomocí $\mini{\widehat{{\sigma}}}\sqrt{v_j}$, dostaneme definici
	 $$ \mathrm{DFBETAS}_{j,i} = \frac{\wbeta_j-\mini{\wbeta}{}_j}{\mini{\widehat{{\sigma}}}\sqrt{v_j}} = \frac{r_{ji}\hei}{\sqrt{v_j}\mini{\widehat{{\sigma}}}(1-\hii)} = \frac{r_{ji}}{\sqrt{v_j}}\frac{\widehat{t}_i}{\sqrt{1-\hii}}, $$
	kde $\widehat{t}_i$ je ext. studentizované reziduum. Kombinuje efekt velkého rezidua $\widehat{t}_i$ a~velkého $\hii$. Jedna možnost pro~limitní hodnoty: $i$-té pozorování je považováno za~influenční na~oblasti $\beta_j$, pokud
	 $$ |\mathrm{DFBETAS}_{j,i}|>\frac{2}{\sqrt{n}}. $$ Máme ovšem velké množství hodnot pro srovnání -- celkem $(m+1)\times n$. Proto tuto metodu zjednodušíme.

	\item[b) Vliv $i$-tého pozorování na~celý vektor $\wbetab$ :]
	spočívá v~použití nějaké normy na~vektor \linebreak $\wbetab-\mini{\wbetab}$. Cook navrhnul
	 $$ D_i = \frac{(\wbetab-\mini{\wbetab})^T\textbf{M}(\wbetab-\mini{\wbetab})}{(m+1)c}, $$
	kde $\textbf{M}$ je PD matice a~$c$ normalizační konstanta. Nejužívanější volbou je $\textbf{M} = \textbf{X}^T\textbf{X}$ a~$c = s_n^2$. Cookova vzdálenost se~potom spočítá jako
	 $$ D_i = \frac{(\wbetab-\mini{\wbetab})^T\textbf{X}^T\textbf{X}(\wbetab-\mini{\wbetab})}{(m+1)s_n^2}. $$
	Dosazením dostaneme
	 $$ D_i = \frac{1}{(m+1)s_n^2}\Big(\frac{\hei}{1-\hii}\Big)^2 \underbrace{\x_i^T\core \overbrace{\X^T\X\core}^I\x_i}_{\hii} = \frac{1}{m+1}\frac{\hii}{1-\hii}\underbrace{\frac{\hei^2}{s_n^2(1-\hii)}}_{=\widehat{r}_i^2}. $$
	Výpočetní formule je potom ve~tvaru
	 $$ D_i = \frac{\widehat{r}_i^2}{m+1}\Big(\frac{\hii}{1-\hii}\Big). $$
\end{description}

\begin{remark}
	 $100(1-\alpha)\%$ simultání IS pro~$\betab$ je $$ C(\alpha) = \Big\{ ~\betab~\Big|\frac{(\wbetab-\betab)^T\X^T\X(\wbetab-\betab)}{(m+1)s_n^2}\leq \FF_{1-\alpha}(m+1,n-m-1) \Big\}, $$
	tzn. $$ \mini{\wbetab}\in C(\alpha)\quad \Leftrightarrow\quad D_i\leq \FF_{1-\alpha}(m+1,n-m-1). $$
	To je motivace pro~\textbf{RULE OF THUMB}: $$ \text{ $i$-té pozorování je influenční, jestliže $D_i>\FF_{\frac{1}{2}}(m+1,n-m-1)$ }.$$
	Pro většinu $m,n$ je $\FF_{\frac{1}{2}}\approx1$, pravidlo tak lze zjednodušit na $D_i>1$.
\end{remark}

\begin{remark}
	Také platí, že
	 $$ D_i = \frac{(\hyb-\mini{\hyb})^T(\hyb-\mini{\hyb})}{(m+1)s_n^2},$$
	tzn. $D_i$ se dá chápat jako míra influence na~celkovou predikci.
\end{remark}

\subsection{DFFITS}
Zavedeme vliv $i$-tého pozorování na~$\hy_i$ jako
 $$ \mathrm{DFFITS}_i = \frac{\hyi-\mini{\hy}}{\mini{\widehat{{\sigma}}}\sqrt{\hii}} = \dots = \widehat{t}_i\sqrt{\frac{\hii}{1-\hii}}. $$
\textbf{RULE OF THUMB:} $i$-té pozorování je influenční, pokud $|\mathrm{DFFITS}|>3\sqrt{\dfrac{m+1}{n-m-1}}$.

\begin{remark}Míry influence v~R:
\begin{itemize}
	\item DFBETAS -- \verb|dfbetas()|
	\item DFFITS -- \verb|dffits()|
	\item Cookova vzdálenost $D_i$ -- \verb|cooks.distance()|
	\item Leverage $\hii$ -- \verb|hatvalues()|
	\item a~vše shrnuje funkce \verb|influence.measures()| (má navíc covariance ratio)
\end{itemize}
\end{remark}

\textbf{Shrnutí používaných pravidel:} $i$-té pozorování je influenční, pokud
$$|\mathrm{DFBETAS_{ij}} | > 1, \quad |\mathrm{DFFITS_i}| > 3 \sqrt{\frac{m+1}{n-m-1}},$$
$$ D_i > F_{\frac{1}{2}}(m+1,n-m-1), \quad \hii > 3 \frac{m+1}{n}.$$


\section{Transformace}

Pokud není splněný některý z~předpokladů modelu: linearita, normalita chyb, homoskedasticita, jednou z~možností je pokusit se~transformovat nějaké proměnné, aby transformovaný model tyto předpoklady alespoň \uv{přibližně} splňoval.

\subsection{Transformace vysvětlované proměnné $y$ }

Hledáme funkci $h(.)$ tak, aby model $Y_i^* = h(Y_i) = \beta_0 + \sumjm x_{ij} \beta_j + e_i$ splňoval předpoklady.

\subsubsection{3 hlavní důvody pro~transformaci $Y$ }
\begin{enumerate}
	\item Transformace škály měření tak, aby pokrývala celé $\R$, což může odstranit problémy s~podmínkami na~$\betab$.
	
	Např. studie kapacity plic (FEV data, FEV $>$ 0):
	\begin{itemize}
		\item Chtěli bychom, aby model nepredikoval záporné hodnoty ($\Rightarrow$ restrikční podmínky na~parametr $\beta$).
		\item Lze obejít modelování $y^* = \ln \mathrm{FEV}$.
	\end{itemize}

	Pokud $y$ jsou počty a~0 je možná hodnota, často se~používá $y^* = \ln (y+1)$ nebo obecně $y^* = \ln(y + c)$
	
	\item Transformace $Y$, aby její rozdělení bylo \uv{více} normální.
	
	Typicky to znamená pokusit se~udělat rozdělení hodnot $y$ více symetrické. Často se~setkáváme s~rozděleními vychýlenými vpravo (obvykle se~to stává, pokud naměříme nějakou fyzikální veličinu, která může nabývat pouze kladných hodnot).
	
	Transformace $y^* = \ln y$ nebo $y^* = y^{\lambda}, \lambda < 1$ budou redukovat toto vychýlení.
	
	Typický postup: Začít s~hodnotou $\lambda$ blízko 1, pak snižovat hodnotu $\lambda$, dokud není dosaženo \uv{přibližně} symetrie reziduí.
	
	\item Možná nejzásadnější motivace je pokusit se~dosáhnout konstantního rozptylu přes všechna pozorování.
	
	Např. pro~fyzikální veličinu s~kladnými hodnotami se~často stane, že rozptyl bude malý pro~$\mu \approx 0$ a~větší pro~$\mu$ velké (už jen z~důvodu, že obor hodnot $y$ je omezen na~kladné hodnoty). Říkáme tomu \textbf{positive mean-variance relationship}.
	
  Nepřesnost měření kladných veličin se~také často vyjadřuje pomocí koeficientu variace
 $$
 \text{CV}(\text{Y}) = \frac{\text{s.d.} \text{Y}}{\E[\text{Y}]}.
 $$
Často bývá více konstantní mezi~případy, než s.d. Variabilitu vyjadřuje relativně, spíše než absolutně. Matematicky to znamená, že $\D[\text{Y}] = \phi \E[\text{Y}]^2 = \phi \mu^2$ pro~nějaké $\phi$.

Pro odstranění vztahu $\E [\text{Y}]$ a~$\D[\text{Y}]$ se~často používají mocninné transformace $y^* = y^{\lambda}$ (pro $y > 0$).

\begin{table}[h]
\centering
 $ \begin{array}{ *{13}{c} }
\text{Transformace:} & \leftarrow &\dots &  y^3 &  y^2 &  y & \sqrt{y}  & \ln y & \frac{1}{\sqrt{y}}  & \frac{1}{y} & \frac{1}{y^2} &\dots  & \rightarrow \\
\text{Box-Cox } \lambda : &\leftarrow  &\dots & 3 & 2 & 1 & \frac{1}{2}  & 0 &  -\frac{1}{\sqrt{2}} & -1 & -2 &\dots & \rightarrow
\end{array} $
\end{table}

\begin{itemize}
	\item $\leftarrow$: Pokud $\D[\text{Y}]$ klesá s~rostoucí $\E[\text{Y}]$, budeme zvyšovat mocninu $\lambda$.
	\item $\rightarrow$: Pokud $\D[\text{Y}]$ roste s~rostoucí $\E[\text{Y}]$, budeme $\lambda$ snižovat.
\end{itemize}

\end{enumerate}


OBECNĚ: Předpokládejme vztah $\D[Y] = \phi \text{V}(\mu)$ a~uvažujeme transformaci $y^* = \text{h}(y)$. Taylorův rozvoj 1. řádu funkce $\text{h}(y)$ v~bodě $\mu$
 $$
 y^* = \text{h}(y) \approx \text{h}(\mu) + \text{h}'(\mu)(y-\mu)
 $$
z čehož plyne, že $\D[\text{Y}^*] \simeq \big(\text{h}'(\mu)\big)^2 \cdot \D[\text{Y}]$.
Transformace $y^* = \text{h}(y)$ tedy bude přibližně stabilizovat rozptyl, pokud $\text{h}'(y)$ je úměrné $\D[\text{Y}]^{-1/2} = \text{V}^{-1/2}(\mu)$.

\begin{itemize}
\item Pokud $\text{V}(\mu) = \mu^2 \quad \Rightarrow \quad$ stabilizující transformace je $\ln (y) = \text{h}(y)$, protože $\text{h}'(y) = \frac{1}{\mu}$.
\item Pokud $\text{V}(\mu) = \mu \quad \Rightarrow \quad$ stabilizující transformace je $\text{h}(y) = \sqrt{y}$, protože $\text{h}'(y) = \frac{1}{2 \sqrt{\mu}}$.
 $$
 \left(\text{h}(\mu) = \int \frac{\d \mu}{\sqrt{\text{V}(\mu)}} \right)
 $$
\item Asi nejvíce užívanou transformací je $y^* = \ln (y)$. Jedním z~důvodů je i~dobrá interpretovatelnost parametrů $\betab$.
\end{itemize}

\begin{remark}
Interpretace parametrů LM

\begin{enumerate}
\item Klasický LM:
 $$
 \E [Y] = \beta_0 + \beta_1 x_1 +\dots + \beta_m x_m.
 $$
Jednotková změna proměnné $x_j \Rightarrow$ změnu $\E [Y]$ o~$\beta_j$ jednotek (při ostatních proměnných stálých).

 $$
\begin{matrix}
\X = (1,x_1,\dots,x_m) & \X_{\text{new}} = (1,x_1,\dots,x_j+1,\dots,x_m) \\
\downarrow &  \downarrow \\
\E [Y] & \E [Y_{\text{new}}] \\
\end{matrix}
 $$
 \vspace{0.3cm}
 $$ \Rightarrow \E [Y_{\text{new}}] - \E [Y] = \beta_j $$
\item LM pro~$\ln Y$ :
 $$
 \ln Y = \beta_0 + \beta_1 x_1 +\dots + \beta_m x_m + e, \quad \text{kde } e \sim \NN(0,\sigma^2).
 $$
Pokud je to správný model, znamená to, že $\ln Y \sim \NN(\mu,\sigma^2) \Rightarrow Y \sim \mathcal{LN}(\mu,\sigma^2)$, a~tedy $\E[Y] = \e{\mu + \frac{\sigma^2}{2}}$. \\
- Predikce pro~$\E [\ln Y]$ je $\widehat{\mu} = \wbeta_0 + \wbeta_1 x_1 +\dots + \wbeta_m x_m$. \\
- Predikce pro~$\E [Y]$ bude $\e{\wbeta_0 + \wbeta_1 x_1 +\dots + \wbeta_m x_m + \frac{\wsigma^2}{2}}$.

Uvazujme opět jednotkovou změnu p. $x_j$ ($x_j \rightarrow x_j + 1$):
 $$
\frac{ \E [Y_{\text{new}}]}{\E [Y]} = \frac{\e{\wbeta_0 + \wbeta_1 x_1 +\dots + \wbeta_j x_j + \wbeta_j +\dots + \wbeta_m x_m + \frac{\sigma^2}{2}}}{\e{\wbeta_0 + \wbeta_1 x_1 +\dots + \wbeta_m x_m + \frac{\sigma^2}{2}}} = \e{\wbeta_j}
 $$
Pak jednotková změna proměnné $x_j \Rightarrow$ multiplikativní změna $\E [Y] \e{\wbeta_j}\text{-krát}$. Jinak zapsáno: $100(\e{\wbeta_j}-1)$ je procentní změna $\E [Y]$ spojená s~jednotkovou změnou $x_j$
\end{enumerate}
\end{remark}

\subsection{Box-Cox transformace}
\begin{itemize}
\item Pokud chyby nemají normální rozdělení, hledáme transformaci $Y$, která by nejenom linearizovala model, ale také transformovala chyby, aby byly přibližně normální.
\item Jako užitečná se~ukazuje následující třída transformací (power family):
 $$
 y^{(\lambda)} = \begin{cases}
      \frac{y^{\lambda}-1}{\lambda}, & \text{pokud}\ \lambda \neq 0 \\
      \ln y, & \text{pokud}\ \lambda = 0
    \end{cases},
 $$
které předpokládají, že data $y$ jsou pouze kladná. (Pokud ne, můžeme přičíst konstantu ke~všem pozorováním a~analyzovat takto posunutá data.)
\begin{remark}
 $\lim_{\lambda \rightarrow 0}  \frac{y^{\lambda}-1}{\lambda} = \ln y$
\end{remark}
\item Pro~nalezení vlastního $\lambda$ budeme předpokládat, že transformované veličiny \\ $Y_i^{(\lambda)}, i\in\widehat{n},$ splňují postačující podmínky RM, tj.
 $$
 Y_i^{(\lambda)} = \x_i^T \bbeta + \eb, \quad \text{kde} \quad \eb \sim \NN_n (\nula,\sigma^2 \Identita{n}) \quad \quad \quad \left(Y_i^{(\lambda)} \sim \NN (\X_i^T \bbeta, \sigma^2) \right)
 $$
\end{itemize}
Úkol je odhadnout zároveň $\lambda, \bbeta, \sigma^2$, použijeme MLE. Pomocí transformace získáme hustotu
 $$
  f_{Y_i}(y_i) = f_{Y_i^{(\lambda)}}(y_i^{(\lambda)}) \cdot \frac{\d y_i^{(\lambda)}}{\d y_i} = \frac{1}{\sqrt{2 \pi \sigma^2}} \e{-\frac{1}{2 \sigma^2}(y_i^{(\lambda)} - \mu_i)^2} \cdot y_i^{\lambda - 1}, \quad \text{kde} \; \mu_i = \E[Y_i^{(\lambda)}] = \x_i^T \bbeta.
 $$
Věrohodnostní funkce pro~pozorování $y_1,\dots,y_n$ bude mít tvar
 $$
  L = \prod_{i = 1}^nf_{Y_i}(y_i) = \left(\frac{1}{\sqrt{2 \pi \sigma^2}} \right)^2 \e{-\frac{1}{2 \sigma^2}(\sumin (y_i^{(\lambda)} - \mu_i)^2} \cdot \mathrm{J}(\lambda), \quad \text{kde} \quad \mathrm{J}(\lambda) = \prod_{i = 1}^n y_i^{\lambda - 1} = \left(\prod_{i = 1}^n y_i \right)^{\lambda - 1}
 $$

Dále vyjádříme log-likelihood
 $$
 l = \ln L = -\frac{n}{2}\ln 2 \pi \underbrace{- \frac{n}{2}\ln \sigma^2 - \frac{1}{2 \sigma^2} \sumin (y_i^{(\lambda)} - \overbrace{\mu_i}^{\x_i^T \wbetab(\lambda)})^2}_{\approx \, l \text{ pro LM s } \y^{(\lambda)} = (y_1^{(\lambda)}, \dots, y_n^{(\lambda)})} + \ln\text{J}(\lambda).
 $$
Věrohodnostní rovnice nemají explicitní analytické řešení. Pro~nalezení MLE si všimneme, že pro~pevné $\lambda$ je $l$ proporcionální logaritmus věrohodnosti pro~odhad ($\bbeta, \sigma^2$) na~základě $\y^{(\lambda)} = (y_1^{(\lambda)},\dots, y_n^{(\lambda)})^T$ v klasickém lineárním modelu $\y^{(\lambda)} = \X^T \betab(\lambda)$. Ten umíme pro pevné $\lambda$ maximalizovat a získat tak odhady
 \begin{align*}
 \wbetab (\lambda) & = \core \X^T \y^{(\lambda)}, \\
\wsigma^2 (\lambda) & = \frac{1}{n} \sumin \left(y_i^{(\lambda)} - \hy _i^{(\lambda)}\right)^2 = \frac{1}{n} (\y^{(\lambda)})^T (\I _n - \Hm) \y^{(\lambda)}, \quad \text{kde} \; \hy_i^{(\lambda)} = \x_i^T \bbeta^{(\lambda)}.
 \end{align*}

Dosazením do~$l$ dostaneme po úpravě hodnotu maximalizovanou vzhledem k~($\bbeta, \sigma^2$), tzv. \textbf{profite log--likelihood}
 $$
  l_p^{(\lambda)} = -\frac{n}{2} \ln 2 \pi - \frac{n}{2} \ln \wsigma^2 (\lambda) - \frac{n}{2} + \ln \text{J}(\lambda)
 = C - \frac{n}{2} \ln \wsigma^2 (\lambda) + (\lambda - 1)  \sumin \ln y_i.
 $$
\begin{remark}
Kvůli komplikované závislosti $l_p$ na~$\lambda$ bude třeba numerická metoda pro~maximalizaci. Lze přepsat do~tvaru, kde bude možné využít metody LR:
\begin{align*}
 l_p (\lambda) & = C - \frac{n}{2} \ln \wsigma^2 (\lambda)  - \frac{n}{2} \ln\text{J}(\lambda)^{2/n} = C - \frac{n}{2} \ln \frac{ \wsigma^2 (\lambda)}{(\text{J}^{\frac{1}{n}}(\lambda))^2} \\
 \text{J}^{\frac{1}{n}}(\lambda) & = \left[\left(\prod_{i = 1}^n y_i \right)^{\frac{1}{n}} \right]^{\lambda - 1} = (\dot{y})^{\lambda - 1}, \quad \text{kde } \dot{y} \text{ je geometrický průměr.}
\end{align*}
\end{remark}
Dosazením zpátky do~$l_p(\lambda)$ dostáváme
\begin{align*}
\quad  l_p(\lambda) & = C - \frac{n}{2} \ln \frac{ \wsigma^2 (\lambda)}{\left[(\dot{y})^{\lambda - 1}\right]^2} = C - \frac{n}{2}\ln s_{\lambda}^2, \\
\text{kde} \quad s_{\lambda}^2 & = \frac{ \wsigma^2 (\lambda)}{\left[(\dot{y})^{\lambda - 1}\right]^2} = \frac{1}{n} \sumin \left(\frac{y_i^{(\lambda)}}{(\dot{y})^{\lambda-1}} - \frac{\hy_i^{(\lambda)}}{(\dot{y})^{\lambda-1}} \right)^2.
\end{align*}
Tedy $s_{\lambda}^2$ je reziduální součet čtverců ($\frac{1}{n}\SSE$) v~modelu $\dfrac{y_i^{(\lambda)}}{(\dot{y})^{\lambda-1}}$ v~závislosti na~$\x_i^T$ (tzn. $s_{\lambda}^2$ lze snadno získat pomocí funkce \verb|lm()|).

Celkem máme vztah
$$ \max_{\lambda} l_p(\lambda) \quad \Leftrightarrow \quad \min_{\lambda} s_{\lambda}^2.$$

\subsubsection{Algoritmus pro~hledání vhodného $\lambda$}
\begin{enumerate}
\item Zvolit oblast hodnot $\lambda$, I~ = $[\lambda_{min}, \lambda_{max}]$, a~body $\lambda \in$ I. Typickou volbou je I~ = $[-2,2]$ a~10--20 rovnoměrně rozdělených bodů).
\item Naladit model $\dfrac{y^{(\lambda)}}{(\dot{y})^{\lambda-1}} \sim \x$ a~spočítat $\frac{1}{n}\SSE = s_{\lambda}^2$.
\item Z~grafu ($\lambda, s_{\lambda}^2$) vybrat $\widehat{\lambda}$, které minimalizuje $s_{\lambda}^2$.
\item Pro~zvolené $\widehat{\lambda}$ naladit model $y^{(\widehat{\lambda})} \sim \x$ a~pokračovat standardní analýzou.
\end{enumerate}

\subsubsection{IS pro~$\lambda$ }
Snadno lze odvodit LRT test pro~test $\text{H}_0 : \lambda = \lambda_0$. Testujeme $\text{H}_0 : \lambda = 1$, tedy zda je třeba transformace. Pokud zamítneme $\text{H}_0$, provedeme transformaci pomocí $\widehat{\lambda}$.

LRT statistika má tvar $$\Lambda = -2 \ln \frac{L(\lambda_0)}{L(\widehat{\lambda})} = 2 \left(l_p(\widehat{\lambda}) - l_p(\lambda_0)\right)$$ a víme, že $\Lambda \Lp \chi^2(1)$. Invertováním příslušné oblasti LRT testu, dostaneme asymptotický $100(1-\alpha)\%$ IS pro~$\lambda$ :
\begin{align*}
 \chi^2_{1-\alpha} & \geq \Lambda \\
\chi^2_{1-\alpha} & \geq 2\left(\frac{n}{2}\ln s^2_{\lambda_0} - \frac{n}{2}\ln s^2_{\widehat{\lambda}}\right) \\
\chi^2_{1-\alpha} & \geq n \ln\frac{s^2_{\lambda_0}}{s^2_{\widehat{\lambda}}}
\end{align*}

Pokud $\widehat{\lambda}$ je MLE $\lambda$, asymptotický $100(1-\alpha)\%$ IS  pro~$\lambda$ je:
 $$
 \left\{ \lambda \in \R | n \cdot \ln\frac{s^2_{\lambda_0}}{s^2_{\widehat{\lambda}}} \leq \chi^2_{1-\alpha}(1) \right\}.
 $$
\begin{remark}
 Kvůli~jednoduchosti interpretace se~často doporučuje zaokrouhlit $\widehat{\lambda}$ na~nejbližší $\frac{1}{4}$ nebo $\frac{1}{3}$.
\end{remark}
\begin{example}
Příklad data TREES
\end{example}

\subsection{Transformace vysvětlujících proměnných x}

Pokud diagnostika modelu naznačuje, že vztah mezi~$\y$ a~$\X$ není lineární pro~jeden nebo více regresorů, může být vhodné přeformovat model pomocí transformací proměnných $\x$.

Předpokládejme, že v~modelu $$Y = \beta_0 + \sumjm \beta_j x_j + e$$ máme podezření na~nelinearitu v~j-té proměnné $x_j$. Jednou z~možností jak postupovat, je nahrazení $x_j$ proměnnou $z_j = f(x_j)$, model dostane podobu $$Y = \beta_0 + \beta_1 x_1 +\dots + \beta_j z_j +\dots + \beta_m x_m + e.$$

Pokud je $f$ známé, jedná se~o~model LR a~lze ho analyzovat standardně. Je-li tato transformace vhodná, mělo by se~to projevit ve~zlepšení statistik $\RMR^2, \mathrm{t}, \text{F}$ a~zlepšení grafu reziduí pro~$z_j$ oproti~těm pro~$x_j$. Bohužel $f$ většinou známá není. Možný přístup je parametrizovat nějak tuto funkci a~pak odhadnout tyto parametry společně s~$\bbeta$.

Typická parametrizace je
 $$
  z_j = x_j^{\lambda}, \quad \text{kde } \lambda \in \R \quad \text{vhodné}.
 $$
Pokud $x_j > 0$, potom $\lambda \in \R$, nicméně pokud může být $x_j$ záporné, je množina hodnot $\lambda$ omezená.

Lze také použít aproximaci $f$ pomocí polynomu vhodného stupně, tzn.
 $$
  z_j = \sum_{k = 1}^l r_k x_j^k, \quad \text{kde } r_k \text{ musí být odhadnuty.}
 $$
 
Další možností je použití trigonometrických funkcí nebo splines (piecewice polynomials). Výsledný model ale v~tomto případě nebude lineární v~parametrech $\beta_j, j = 0,\dots,m$ \linebreak a~$ r_k, k~ = 1,\dots,l $.

\subsubsection{Zaměříme se~na~$z_j = x_j^\lambda$}
\begin{itemize}
	\item Možnost je opět zvolit jistou množinu hodnot $\lambda$, naladit modely pro~všechna $\lambda$ a~vybrat model s~nejlepší shodou s~daty, např. s~nejmenší $\SSE$ nebo největší $\RR^2$ nebo $F$.
	\item Problémy: Může být časově náročné, můžeme minout vhodnou hodnotu $\lambda$, pokud nebyla v~původní množině (nevíme jak $\RR^2,F,\SSE$ závisí na~$\lambda$).
\end{itemize}

\subsubsection{Box-Tidwell metoda}
Předpokládejme, že $\lambda$ se~příliš neliší od~$\lambda = 1$. Taylorův rozvoj 1. řádu kolem~$\lambda = 1$ dává
\begin{align*}
x^\lambda & \approx x^1+(\lambda-1)\frac{\d x^\lambda}{\d \lambda}\Big|_{\lambda = 1}, \quad \text{kde } \frac{\d x^\lambda}{\d \lambda}\Big|_{\lambda = 1} = x^\lambda \ln x \Big|_{\lambda = 1} = x\ln x \\
x^\lambda & \approx x+(\lambda-1)x\ln x.
\end{align*}
Dosazením do~modelu
\begin{align*}
	Y & = \beta_0+\beta_1 x_1+\dots+\beta_{j-1}x_{j-1}+\beta_j\big(x_j+(\lambda-1)x_j\ln x_j\big)+\dots+\beta_m x_m+e = \\
	& = \beta_0+\sum_{k = 1}^m \beta_k x_k+\underbrace{\beta_j(\lambda-1)}_{\beta_{m+1}(\lambda)}x_j\ln x_j+e
\end{align*}
získáme lineární model pro~parametry $\beta_k$, $0\leq k\leq m+1$, a protože $\beta_{m+1} = (\lambda-1)\beta_j$, můžeme $(\lambda,\beta_j)$ odhadnout následovně:
\begin{enumerate}[1)]
	\item naladíme původní model a~spočteme $\LSE$ $\wbeta_j$ parametru $\beta_j$,
	\item naladíme rozšířený model s~$x_{m+1} = x_j\ln x_j$ a~spočteme $\wbeta_{m+1}$,
	\item z~rovnosti $\wbeta_{m+1} = (\widehat{\lambda}-1)\wbeta_j$ dostaneme $\widehat{\lambda} = \frac{\wbeta_{m+1}}{\wbeta_j}+1$.
\end{enumerate}
Tento postup umožňuje testovat potřebu transformace
 $$ \hypothesis{\lambda = 1}{\lambda\neq1} $$
pomocí t-testu pro~$H_0:~\beta_{m+1} = 0$.

\begin{remark}
Pokud model s~$\widehat{\lambda}$ vypadá neadekvátně, lze postupovat iterativně a~získat posloupnost $\widehat{\lambda}(l),~l\geq1$. Položíme $\widehat{\lambda}(0) = \widehat{\lambda}$ a~rozvineme $x_j^\lambda$ kolem~$\widehat{\lambda}(0)$, tzn. $$x_j^\lambda\approx x_j^{\widehat{\lambda}(0)}+\big(\lambda-\widehat{\lambda}(0)\big)x_j^{\widehat{\lambda}(0)}\ln x_j$$ a dosazením do~rovnice modelu
 $$
Y = \beta_0 + \sum_{\substack{k = 1 \\ k\neq j}}^m \beta_k x_k+\beta_j x_j^{\widehat{\lambda}(0)}+\underbrace{\beta_j\big(\lambda-\widehat{\lambda}(0)\big)}_{\beta_{m+1}}x_j^{\widehat{\lambda}(0)}\ln x_j+e.
 $$
Naladíme tento model s~a~bez~přidané proměnné $x_{m+1} = x_j^{\widehat{\lambda}(0)}\ln x_j$. Označíme $\wbeta_j(1)$ a~$\wbeta_{m+1}(1)$ příslušné odhady. Potom
 $$
\widehat{\lambda}(1) = \widehat{\lambda}(0)+\frac{\wbeta_{m+1}(1)}{\wbeta_j(1)}.
 $$
Můžeme dále iterovat do~konvergence nebo skončit po~pevném počtu iterací.
\end{remark}

\begin{remark}
	Další užívané transformace v~$\x,\y$ :
	\begin{description}
	\item[a) centrované proměnné:] transformujeme $\X$ na $\X_C$ tak, že $(\X_C)_{ij} = x_{ij}-\overline{x}_j,~i\in\widehat{n},~j\in\widehat{m}$, kde $\overline{x}_j = \frac{1}{n}\sumin x_{ij}$ je průměr $j$-tého sloupce matice $\X, (\y_C)_i = y_i-\overline{y}$. Parametry pak odhadneme jako
	\begin{enumerate}[1)]
		\item $\wbeta_1, \dots, \wbeta_m$ jsou řešením $\X_C^T \X_C \betab = \X_C^T \y_C$,
		\item $\wbeta_0 = \ly - \sum_{j=1}^m \wbeta_j \lx_j$.
	\end{enumerate}
	\item[b) centrované a~škálované proměnné:] škálování sloupců tak, aby jejich norma byla 1, tzn. každý prvek $j$-tého sloupce matice $\X$ podělíme $s_j = \Big(\sumin (x_{ij}-\overline{x}_j)^2\Big)^{\frac{1}{2}}$. Centrované a~škálované matice $\X_\mathrm{SC}$ pak bude
	 $$
	\X_\mathrm{SC} = \X_C\textbf{S},\qquad \textbf{S} = \mathrm{diag}\Big(\frac{1}{s_1},\dots,\frac{1}{s_m}\Big).
	 $$
	Model pak bude
	 $$
	\Y_C = \X_\mathrm{SC}\betab_s+\eb. $$ Lze použít i~$\Y_\mathrm{SC}$, tedy centrované a~škálované $\Y$.
	\end{description}
\end{remark}

\section{Vážené nejmenší čtverce (weight least squares WLS)}
Budeme nyní předpokládat, že chyby $e_i$ jsou normální, nezávislé, ale $\D(e_i) = \sigma_i^2$ závisí na~$i$. Konkrétně tedy $\sigma_i^2 = \frac{\sigma^2}{w_i}$, kde $w_i>0,~i\in\widehat{n}$ se~nazývají váhy.

Uvažujeme tedy model
\begin{equation}\label{rovnickaW}
\Y = \X\betab+\eb,\text{ kde }\eb\sim\NN_n(\nula,\sigma^2\Wm)\text{ a~}\Wm = \mathrm{diag}\Big(\frac{1}{w_1},\dots,\frac{1}{w_n}\Big).
\end{equation}
Pokud jsou váhy $w_i$ známé, lze MLE odhady parametru $\betab$ a~$\sigma^2$ nalézt následovně:

Označíme
 $$
\Wm = \Km \Km^T,\text{ kde }\Km = \Wm^{\frac{1}{2}} = \mathrm{diag}\Big(\frac{1}{\sqrt{w_1}},\dots,\frac{1}{\sqrt{w_n}}\Big)
 $$
a definujeme $\textbf{Z} = \inv{\Km}\Y,~\textbf{M} = \inv{\Km}\X$ a~$\boldsymbol{\epsilon} = \inv{\Km}\eb$. Potom dostaneme model
\begin{equation}\label{rovnickaW2}
\textbf{Z} = \textbf{M}\betab+\boldsymbol{\epsilon},\text{ kde }\boldsymbol{\epsilon}\sim\NN_n(0,\sigma^2 \In),
\end{equation}
protože
 $$
\Cov(\boldsymbol{\epsilon}) = \inv{\Km}\sigma^2\Wm\big(\inv{\Km}\big)^T = \sigma^2 \inv{\Km}\Km\Km^T\big(\Km^T\big)^{-1} = \sigma^2 \In.
 $$

Transformační vektor je tedy ve~tvaru $\textbf{Z} = (\sqrt{w_1}Y_1,\dots,\sqrt{w_n}Y_n)^T$. To už je standardní model LR, na~kterém platí
\begin{align*}
\wbetab_w & = \inv{(\textbf{M}^T\textbf{M})}\textbf{M}^T\textbf{z} = \inv{\big(\X^T\underbrace{(\Km^{-1})^T \inv{\Km}}_{ = \inv{{(\Km\Km^T)}} = \inv{\Wm}}\X\big)}\X^T(\inv{\Km})^T\inv{\Km}\Y = \inv{(\X^T\inv{\Wm}\X)}\X^T\inv{\Wm}\Y, \\
\widehat{{\sigma}^2}_w & = \frac{1}{n}\sumin (z_i-\widehat{z}_i)^2 = \frac{1}{n}\sumin w_i(y_i-\hyi)^2 = \frac{1}{n}\SSE_w,
\end{align*}
 kde $\SSE_w$ je vážený součet čtverců, $z_i = \sqrt{w_i}y_i$ a~$\widehat{z}_i = \sqrt{w_i}\x_i^T\wbetab = \sqrt{w_i}\hyi$.
 
Dále platí, že
\begin{enumerate}[a)]
	\item $\E\wbetab_w = \inv{(\X^T\inv{\Wm}\X)}\X^T\inv{\Wm}\underbrace{\E \Y}_{\X\betab} = \betab$, kde $\wbetab_w$ je nestranný odhad $\bbeta$,
	\item $\E\Big(\frac{\SSE_W}{n-m-1}\Big) = \sigma^2$, tedy $s_w^2 = \frac{\SSE_w}{n-m-1}$ je nestranný odhad $\sigma^2$.
\end{enumerate}

\begin{theorem}
Nechť $\wbetab_w$ je WLS odhad $\bbeta$, $\Cov(\eb) = \sigma^2\Wm = \sigma^2\mathrm{diag}\Big(\frac{1}{w_1},\dots,\frac{1}{w_n}\Big)$. Potom platí, že
\begin{enumerate}[1)]
	\item $\Cov(\wbetab_w) = \sigma^2\inv{(\X\inv{\Wm}\X)}$,
	\item nechť $\delta_i$ je $i$-tý diagonální prvek $\inv{(\X^T\inv{\Wm}\X)}$. Jestliže $e_i\sim \NN\Big(0,\frac{\sigma^2}{w_i}\Big)$, $i\in\widehat{n}$, potom
	 $$ T_i = \frac{\wbeta_{w,i}-\beta_i}{s_w\sqrt{\delta_i}}\sim t(n-m-1), $$
	\item pro~$\widehat{\Y} = \X\wbetab_w$ platí, že $\E \widehat{\Y}_w = \X\betab$ a~$\Cov(\widehat{\Y}_w) = \sigma^2\X\inv{(\X^T\inv{\Wm}\X)}\X^T$,
\item nechť $\heb_w = \Y-\widehat{\Y}_w$ jsou rezidua v~modelu \eqref{rovnickaW} a~$\widehat{\boldsymbol{\epsilon}}_w = \textbf{Z}-\widehat{\textbf{Z}} = \textbf{Z}-\Mm\wbetab_w$ jsou rezidua v~transformovaném modelu \eqref{rovnickaW2}. Potom
 $$ \widehat{\boldsymbol{\epsilon}}_w = \sqrt{\inv{\Wm}}\heb_w = \Wm^{-\frac{1}{2}}\he_w\quad\text{a}\quad \E(\heb_w) = \E(\boldsymbol{\widehat{\epsilon}}_w) = \nula, $$
\item nechť $\Hm_w = \X\inv{(\X^T\inv{\Wm}\X)}\X^T\inv{\Wm}$ je vážená projekční matice. Potom
 $$ \heb_w = (I-\Hm_w)\eb\quad\text{a}\quad \Cov(\heb_w) = \sigma^2(I-\Hm_w)\Wm. $$
To znamená, že $$ \Cov(\widehat{\boldsymbol{\epsilon}_w}) = \sigma^2\Wm^{-\frac{1}{2}}(I-\Hm_w)\Wm^{\frac{1}{2}}. $$
\end{enumerate}

\begin{proof}
	\begin{enumerate}[1)]
		\item $$ \Cov (\wbetab_w) = \inv{(\X^T\inv{\Wm}\X)}\X^T\inv{\Wm}\underbrace{\Cov \Y}_{ = \sigma^2\Wm}\inv{\Wm}\X\inv{(\X^T\inv{\Wm}\X)} = \sigma^2\inv{(\X^T\inv{\Wm}\X)}, $$
		\item $\D \wbeta_{w,i} = \sigma^2\delta_i$, tzn. $\frac{\wbeta_{w,i}-\beta_i}{\sigma\sqrt{\delta_i}}\sim\NN(0,1)$ a~víme, že $\wbeta_{w,i}$ a~$s_w^2$ jsou nezávislé, $\frac{s_w^2(n-m-1)}{\sigma^2}\sim\chi^2(n-m-1)$. Z~toho vyplývá, že
		 $$ \frac{\wbeta_{w,i}-\beta_i}{s_w\sqrt{\delta_i}}\sim t(n-m-1). $$
		\item $\E \widehat{\Y}_w = \X\E \wbetab_w = \X\betab$, $\Cov(\widehat{\Y}_w) = \X\Cov\wbetab_w \X^T = \sigma^2\X\inv{(\X^T\inv{\Wm}\X)}\X^T$.
		\item Protože $\textbf{Z} = \Wm^{-\frac{1}{2}}\Y$ a~$\textbf{M} = \Wm^{-\frac{1}{2}}\X$, dostaneme
		\begin{align*}
		\widehat{\boldsymbol{\epsilon}}_w & = \textbf{Z}-\widehat{\textbf{Z}} = \Wm^{-\frac{1}{2}}\Y-\Wm^{-\frac{1}{2}}(\Y-\X\wbetab_w) = \Wm^{-\frac{1}{2}}\heb_w,\\
		\E \heb_w & = \E(\Y-\widehat{\Y}_w) = \X\betab-\X\betab = \nula\quad\Rightarrow\quad \E \widehat{\boldsymbol{\epsilon}} = \nula.
		\end{align*}
		\item \[
		\begin{split}
		\heb_w& = \Y-\widehat{\Y}_w = \Y-\X\wbetab_w = \X\betab+\eb-\X\inv{(\X^T\inv{\Wm}\X)}\X^T\inv{\Wm}\underbrace{(\X\betab+\eb)}_{\Y} = \\& = \X\betab-\X\betab+\eb-\underbrace{\X\inv{(\X^T\inv{\Wm}\X)}\X^T\inv{\Wm}}_{\Hm_w}\eb = (I-\Hm_w)\eb,\\
		\Cov(\heb_w)& = (I-\Hm_w)\Cov~\eb(I-\Hm_w)^T = \\& = \sigma^2(I-\X\inv{(\X^T\inv{\Wm}\X)}\X^T\inv{\Wm})\Wm\big(I-\inv{\Wm}\X\inv{(\X^T\inv{\Wm}\X)}\X^T\big) = \\& = \sigma^2\Wm-\sigma^2\X\inv{(\X^T\inv{\Wm}\X)}\X^T-\sigma^2\X\inv{(\X^T\inv{\Wm}\X)}\X^T
+\sigma^2\X\inv{(\X^T\inv{\Wm}\X)}\X^T = \\& = \sigma^2(I-\Hm_w)\Wm.	\end{split}
		\]
		 $\Cov(\widehat{\boldsymbol{\epsilon}}) = \Wm^{-\frac{1}{2}}\Cov(\widehat{\boldsymbol{\epsilon}})\Wm^{-\frac{1}{2}} = \sigma^2\Wm^{-\frac{1}{2}}(I-\Hm_w)\Wm^{\frac{1}{2}}$.
	\end{enumerate}
\end{proof}
\end{theorem}

Z~dosazení vyplývá, že odhady parametru $\betab$ a~$\sigma^2$ lze získat použitím transformovaného modelu \eqref{rovnickaW2}. Protože ale transformovaný model neobsahuje intercept (první sloupec M je $(\sqrt{w_1},\dots, \sqrt{w_n})^T$), nefunguje klasický rozklad součtu čtverců a~F statistika nelze definovat obvyklým způsobem, stejně jako $R^2$ (viz. regrese skrz~počátek)

Nicméně princip \uv{extra sum of squares} funguje, ať má model intercept nebo ne: např. celkový F-test lze provést pomocí statistiky
$$ F_w = \frac{\SSE_R - \SSE_F}{\frac{m}{s_w^2}}, $$
kde $\SSE_F$ je reziduální součet čtverců $s_w^2$ plného modelu a~$\SSE_R$ je reziduální součet čtverců redukovaného transformovaného modelu $\textbf{Z} = \Mm_0 \betab_0 + \eb$, $\Mm_0 = (\sqrt{w_1},\dots, \sqrt{w_n})^T)$.
	
Pokud mají chyby normální rozdělení, pak
$$\text{za platnosti } H_0: \beta_1 = \dots = \beta_m = 0 \quad \Rightarrow \quad F_w \sim F(m, n-m-1)$$
a~$H_0$ zamítáme, pokud $F_w > F_{1-\alpha}(m,n-m-1)$.
	
Přirozené je definovat $\RMR^2 = \rho^2(\hzb, \z)$, kde $\rho(\hzb, \z)$ je výběrový korelační koeficient. Pro~$\Wm = \I$ dostaneme standardní $\RMR^2$.
%str 110 (*)
	
\subsection{Analýza reziduí pro WLS}
	Pro~analýzu reziduí je třeba uvažovat vhodné grafy reziduí:
	\begin{itemize}
		\item máme dva vektory reziduí:
		\begin{align*}
			& \hei \text{ v~původním modelu \eqref{rovnickaW}}, \\
			& \heps_i \text{ v~transformovaném modelu \eqref{rovnickaW2}},
		\end{align*}
		a tedy dvě možnosti
		\item pro~kontrolu konstantního rozptylu lze uvažovat i~standardizovaná nebo studentizovaná rezidua (pomocí bodu 4) a~5) věty lze ukázat, že jsou v~obou modelech stejná)
		\item je třeba být opatrný oproti~jakým hodnotám budeme rezidua zobrazovat
		\item grafy $\heps_i$ proti~sloupcům $\Mm$ a~predikovaným hodnotám $\hzb$ jsou OK, neboť např.
		 $$
		\sumin \hz_i \heps_i = 0
		 $$
		(jsou OG, měl by být vidět rozptýlený oblak kolem~osy x).
		\item dosazením $\heps_i = \sqrt{w_i}  \cdot \hei$ a~$\hz_i = \sqrt{w_i} \cdot \hy_i$ dostaneme $\sumin w_i \hei \hy_i = 0$, tzn. graf $\hei$ proti~$\hy_i$ bude zavádějící
		\item graf $\sqrt{w_i}  \cdot \hei$ proti~$\sqrt{w_i}  \cdot \hy_i$ je ale v~pořádku
		\item podobné závěry platí i~pro~grafy $\hei$ proti~$\x_j^c, i~ = 1,\dots, m$.
	\end{itemize}


\begin{remark}
Pokud jsou váhy neznámé, bylo by třeba je odhadnou společně s~$\betab$ a~$\sigma^2$ z~dat. To ale není obecně možné, protože máme více parametrů, než dat. Někdy to možné je, pokud máme další informace o~rozdělení chyb (tvar kovarianční matice atd.).
\end{remark}

\begin{remark}
	Celý postup WLS lze použít i~na~případ $\eb \sim \NN_m(0,\sigma^2 \Wm)$, kde $\Wm$ je známá, ale není diagonální. Protože $\Wm$ je symetrická, ex. regulární $\Km$ tak, že $\Wm = \Km \Km^T$. Stejná transformace jako u~WLS opět vede na~transformovaný model, kde $\epsb \sim \NN_n(0,\sigma^2 \Identita{n})$.
\end{remark}

\section{Korelované chyby}
\begin{itemize}
\item Zejména v~časových nebo ekonomických datech se~často objevuje korelace jednotlivých hodnot.
\item potom není splněn předpoklad nezávislosti chyb
\item tento stav je třeba detekovat (někdy pomohou grafy reziduí)
\item modely pro~korelovaná data: \textbf{Analýza časových řad}
\end{itemize}

Pokud je přítomna autokorelace a~chyby mají konstantní rozptyl, platí, že
\begin{enumerate}
\item OLS odhad $\wbeta$ je nestranný, ale neplatí Gauss-Markovova věta, tzn. $\wbeta$ nemá nejmenší rozptyl.
\item MSE = $\frac{1}{n-m-1}\SSE$ (odhad $\sigma^2$) může být podstatně menší, než skutečná hodnota $\sigma^2$, což může dávat falešný pocit přesnosti.
\item V~důsledku bodu 2) mohou být zvětšeny hodnoty T statistik, takže testy o~parametrech a~IS nefungují.
\item Protože jsou chyby nezávislé, F-testy a~t-testy nejsou přesně platné ani když jsou chyby normální.
\end{enumerate}

\subsection{Durbin-Watson statistika}
Omezíme se~na~pozorování získaná v~čase $t = 1,2,\dots, n$ a~případ, že chyby $e_t$ splňují podmínky autoregresního procesu 1. řádu (AR1), tj.
 $$
e_t = \rho e_{t-1}+ u_t, \quad |\rho| < 1,
 $$
kde $\rho$ je autokorelační koeficient, $u_t \sim \NN (0, \sigma_n^2)$ jsou nezávislé v~$t  \in\widehat{n} $ a~$u_t$ je nezávislá na~$e_t, t \geq 1$. Častěji pro~data časových řad platí $\rho > 0$ (pozitivní autokorelace).

Pro test $\hypothesis{\rho = 0}{\rho > 0}$ se~užívá \textbf{Durbin-Watsonova statistika}
 $$
d = \frac{\sum_{t = 2}^n(\he_t - \he_{t-1})^2}{\sum_{t = 1}^n \he_t^2},
 $$
kde $\he_t$ jsou rezidua modelu LR. Pokud zamítneme $H_0$, odhadne se~$\rho$ pomocí
 $$
\widehat{\rho} = \frac{\sum_{t = 2}^n \he_t \he_{t-1}}{\sum_{t = 2}^n \he_t^2}.
 $$

\newcommand{\sumtn}{\sum_{t = 2}^n}
\begin{remark}
Dá se~ukázat, že $d \approx 2(1-\hrho)$ :
 $$
\sum_{t = 2}^n(\he_t - \he_{t-1})^2 = \sumtn \he_t^2 + \sumtn \he_{t-1}^2 - 2 \sumtn \he_t \he_{t-1} \approx 2 \left(\sumtn \he_t^2 - \sumtn \he_t \he_{t-1} \right),
 $$
Z Cauchy-Schwartzovy nerovnosti $\Rightarrow \frac{\sumtn \he_t \he_{t-1}}{\sum_{t = 1}^n \he_t^2}$ leží přibližně v~$(-1,1)$, tzn. $d$ leží přibližně v~$(0,4)$. Dále
 $$
\hrho \approx 1 \Rightarrow d \approx 0 \quad \text{a} \quad \hrho \approx 0 \Rightarrow d \approx 2,
 $$
tzn. pro~malé hodnoty $d$ budeme zamítat $H_0$, pro~velké hodnoty nebudeme zamítat. Kritické hodnoty určené Durbinem a~Watsonem jsou tabelované.
\end{remark}

\noindent \textbf{Test:}
\begin{enumerate}
	\item spočítat hodnotu $d$
	\item nalézt kritické hodnoty $(d_L,d_U)$ pro~dané $n$ a~$m+1$
	\item \begin{enumerate}
		\item zamítnout $H_0$, pokud $d < d_L$
		\item nezamítnout $H_0$, pokud $d > d_U$
		\item pro~$d_L < d < d_U$ test nerozhodne
	\end{enumerate}
\end{enumerate}

\begin{remark}
	Pro test $\hypothesis{\rho = 0}{\rho < 0}$ lze použít popsaný test pro~$d' = 4 - d$. Metody pro~korekci autokorelace: \textbf{Cochrane-Orcutt}.
\end{remark}

\chapter{Výběr regresního modelu}

Chybí část - asi přednáška 11/12.

\section{Kritéria pro porovnávání modelů}

\begin{enumerate}[A)]
\item
Mallows $C_p$, AIC, BIC

Kritéria beroucí více v~potaz počet použitých regresorů. Lze je použít i~pro~\textbf{nevnořené modely}!
\begin{description}
\item[Mallows $C_p$]
 $$
C_p = \frac{\SSE_p}{\wsigma^2} - n + 2p, \quad \wsigma^2 = \frac{\SSE_T}{n-T}
 $$
\textbf{Vlastnosti $C_p$ }:
\begin{enumerate}[1)]
\item Snadno se~počítá, $\SSE_p$ a~$\wsigma^2$ jsou implementované.
\item Pokud je $\wsigma^2$ konzistentní odhad $\sigma^2$ (nezávisející na~$p$), má $C_p$ následující interpretaci: Porovnává, co zbývá vysvětlit pomocí modelů s~$p$ a~$T$ parametry, zvýhodňuje počet dostupných dat a~penalizuje počet parametrů, které je třeba odhadnout.
\item Při~zvyšování počtu regresorů: $\wsigma^2$ je konstantní, $\SSE_p$ klesá, $p$ roste ($C_p$ se~snaží sladit dvě protichůdná kritéria).
\item $C_T = T$.
\item Pokud je správný model s~$p$ parametry, dá se~ukázat, že $C_p \approx p$ pro~$n >> T$.
\item V~praxi se~volí model s~nejmenším $C_p$ ve~skupině modelů splňujících $C_p \approx p$. (Obrázek)
\end{enumerate}

\begin{remark}
Nevýhoda: Pro dobrou interpretaci je třeba spočítat $C_p$ pro~všechny nebo většinu podmnožin regresorů.
\end{remark}

\item[Akaikeho informační kritérium AIC]

\newcommand{\AIC}{\mathrm{AIC}}
Obecná definice je
 $$
\AIC = -2l(\wtheta) + 2p^*,
 $$
kde $\wtheta$ je MLE odhad v~modelu, $l$ je log-věrohodnostní funkce a~$p^*$ je počet parametrů, které je třeba odhadnout ($p^* = p + 1$, jelikož počítáme i~$\sigma^2$).

Pro náš model LR:
\begin{align*}
L(\betab,\sigma^2) & = \frac{1}{\left(2\pi \sigma^2\right)^{n/2}} \exp \left[- \frac{1}{2\sigma^2} \left(\y - \X \betab \right)^T \left(\y - \X \betab \right) \right] \\
l(\betab, \sigma^2) & = - \frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \sigma^2 - \frac{1}{2} \frac{\left(\y - \X \betab \right)^T \left(\y - \X \betab \right)}{\sigma^2} \\
\AIC & = -2l(\wbetab, \wsigma^2) + 2p^* = n \ln 2\pi + \ln \wsigma^2 + \frac{\left(\y - \X \betab \right)^T \left(\y - \X \betab \right)}{\wsigma^2} + 2p^*,
\end{align*}
ale $\wsigma^2 = \frac{1}{n} \left(\y - \X \betab \right)^T \left(\y - \X \betab \right) = \frac{\SSE}{n}$, tedy
 $$
\AIC = \underbrace{n \ln 2 \pi + n}_C + n \ln \frac{\SSE}{n} + 2p^*
 $$
nebo alternativně $\AIC = n \ln \frac{\SSE}{n} + 2p^*$.

\begin{remark}
\begin{itemize}
\item hledáme model s~minimální hodnotou AIC
\item AIC není mírou kvality modelu, je užitečná pro~porovnávání modelů
\end{itemize}
\end{remark}

\textbf{AIC v~R:}

- \verb|AIC(.)| počítá $\AIC = n \ln 2 \pi + n + n \ln \frac{\SSE}{n} + 2p^*$, kde $p^*$ je počet parametrů $\betab, \sigma^2$ (včetně interceptu) \\
- \verb|extractAIC(.)| počítá $\AIC = + n \ln \frac{\SSE}{n} + 2p^*$, kde $p^*$ je jen počet parametrů $\betab$ (včetně interceptu)

\item[(Schwarzovo) bayesovské informační kritérium BIC]

\newcommand{\BIC}{\mathrm{BIC}}

Z definice
 $$
\BIC = -2 l(\wtheta) + p^* \ln n.
 $$
Více penalizuje počet parametrů $\Rightarrow$ vybírá jednodušší modely s~jednodušší interpretací, než AIC. BIC vyžaduje významnější příspěvek proměnné, aby byla zařazena do~modelu. (AIC se~zanořuje za~predikci, BIC je kompromis mezi~interpretovatelností a~predikcí.)

\textbf{BIC v~R:}

- \verb|BIC(.)| nebo \verb|AIC(.), extractAIC(.)| s~volbou \verb|k = log(nobs(fit))|.

Příklad - data HALD.
\end{description}

\item
PRESS statistika

\newcommand{\PRESS}{\mathrm{PRESS}}

Pokud je pro~nás důležitá kvalita predicve, lze použít pro~srovnání modelů statistiku
 $$
\PRESS = \sumin \he_{(-i)}^2 = \sumin \left(\frac{\hei}{1 - \hii} \right)^2.
 $$
Vybírá se~model s~minimální hodnotou této statistiky.
\end{enumerate}

\section{Metody výběru modelu}

\begin{enumerate}[1)]

\item
\textbf{Vyhodnocení všech možných modelů}

Pro $T$ dostupných regresorů tzn. naladit $2^T$ modelů. Pak je porovnat pomocí nějakého kritéria. Náročné pro~velká $T$ (například $T = 20$ znamená $1024$ modelů).

\item
\textbf{Zpětná eliminace (backward elimination)}

Začneme s~plným modelem a~v~každém kroku odstraníme jednu proměnnou, která nejméně přispívá modelu (měřeno F statistikou) nebo jejíž odstranění znamená největší zlepšení modelu (měřeno AIC).

\textbf{algoritmus:}
\begin{enumerate}[	1)]
	\item Naladíme model se~všemi proměnnými.
	\item Pro~každou proměnnou spočteme částečnou $\FF$ statistiku (nebo $t$-statistiku) jako by právě byla přidána do~modelu, tzn. za~předpokladu, že ostatní proměnné tam už jsou.
	\item Pokud je nějaká $\FF$-statistika menší, než kritická hodnota $\FF_\mathrm{OUT}$, vynecháme z~modelu proměnnou s~nejnižší hodnotou $\FF$. $\FF_\mathrm{OUT} = \FF_{1-\alpha_{\mathrm{OUT}}}(1,n-p)$, kde $p$ je aktuální počet regresorů v~modelu, včetně interceptu, $\alpha_\text{OUT} = 0.05,0.1,\dots$
	\item Opakujeme kroky 2) a~3), dokud všechny částečné $\FF$ statistiky nejsou větší, než příslušná kritická hodnota $\FF_\mathrm{OUT}$, tzn. nelze už vyřadit žádnou proměnnou.
\end{enumerate}
\begin{remark}
	Místo $\FF$ lze foužívat AIC.
\end{remark}

\item
\textbf{Dopředná regrese (forward regression)}

Začneme pouze s~interceptem (nebo nutným minimálním modelem) a~v~každém kroku přidáme jednu proměnnou, která má za~následek největší zlepšení modelu (největší nárůst $\FF$ nebo největší pokles AIC).

Tato metoda neumožňuje odstranit proměnnou, která už do~modelu byla přidána.

\textbf{algoritmus:}\begin{enumerate}[	1)]
	\item Naladíme minimální model.
	\item Pro~každou dostupnou proměnnou spočteme $\FF$ statistiku pro~test významnosti jejího přidání do~modelu.
	\item Pokud některá z~těchto $\FF$ statistik překračuje kritickou hodnotu $\FF_\mathrm{in}$, přidáme do~modelu proměnnou s~nejvyšší hodnotou $\FF$ statistiky.
	\item Opakujeme kroky 2) a~3), dokud všechny $\FF$-statistiky pro~zbývající proměnné nebudou menší, než $\FF_\mathrm{in}$ nebo dokud nezbyde žádná proměnná na~přidání do~modelu.
\end{enumerate}
\begin{remark}
	I když tento postup zjednodušuje výběr modelu, často bohužel vede na~zařazení proměnných, které nemají významný příspěvek, jakmile jsou zařazeny další proměnné.
\end{remark}

\item
\textbf{Postupná regrese (stepwise regression)}

Kombinace dvou předchozích metod. V~každém kroku algoritmu přidáváme jednu proměnnou a~poté zkontrolujeme, zda není možné nějakou odebrat. Budeme potřebovat  dvě kritické hodnoty $\FF_\mathrm{in}$, $\FF_\mathrm{OUT}$ (pro použití $\FF$ statistiky).

\textbf{algoritmus:}\begin{enumerate}[	1)]
	\item Naladíme minimální model.
	\item Zjistíme, zda přidání nějaké další proměnné může zlepšit model ($\FF$ nebo AIC). Pokud ano, přidáme do~modelu proměnnou, která má za~následek největší zlepšení modelu (nebo největší pokles AIC).
	\item V~novém modelu zjistíme, zda nelze některou proměnnou vynechat (opět pomocí AIC nebo $\FF$). Pokud ano, vynecháme proměnnou, jejíž vyřazení má za~následek největší zlepšení modelu (nebo největší pokles AIC).
	\item Opakujeme kroky 2) a~3) do~té doby, až nebude možné přidat ani ubrat žádnou proměnnou.
\end{enumerate}
\end{enumerate}

\begin{remark}[Princip marginality]
	\begin{itemize}
		\item Pokud jsou v~modelu vyšší mocniny nějakého regressoru, měly by tam být obsaženy i~všechny jeho nižší mocniny (i když jsou případně nevýznamné).
		\item Pokud je v~modelu obsažena interakce dvou regressorů, měly by tam být i~oba individuální regresory.
		\item S~každou interakcí vyššího řádu by měl model obsahovat i~všechny interakce řádu nižšího. ($a:b:c~\to~a:b,a:c,b:c$).
	\end{itemize}
\end{remark}

\begin{remark}
	Jakmile nalezneme optimální model, je třeba řádně ověřit adekvátnost.
\end{remark}

\chapter{Kolinearita (multikolinearita)}
TBD
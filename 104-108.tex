\textbf{Zaměříme se na $z_j=x_j^\lambda$:}
\begin{itemize}
	\item možnost je opět zvolit jistou množinu hodnot $\lambda$, naladit modely pro všechna $\lambda$ a vybrat model s nejlepší shodou s daty, např. s nejmenší $\SSE$ nebo největší $\RR^2$ nebo $F$
	\item může být časově náročné, můžeme minout vhodnou hodnotu $\lambda$, pokud nebyla v původní množině (nevíme jak $\RR^2,F,\SSE$ závisí na $\lambda$)
\end{itemize}

\subsubsection*{Box-Tidwell metoda}
Předpokládejme, že $\lambda$ se příliš neliší od $\lambda=1$. Taylorův rozvoj 1. řádu kolem $\lambda=1$ dává
$$
x^\lambda\approx x^1+(\lambda-1)\frac{\d x^\lambda}{\d \lambda}\Big|_{\lambda=1},\qquad \frac{\d x^\lambda}{\d \lambda}=x^\lambda\ln x c \frac{\d x^\lambda}{\d \lambda}\Big|_{\lambda=1}=x\ln x,\qquad x^\lambda\approx x+(\lambda-1)x\ln x.
$$
Dosazením do modelu
\begin{align*}
	Y & =\beta_0+\beta_1 x_1+...+\beta_{j-1}x_{j-1}+\beta_j\big(x_j+(\lambda-1)x_j\ln x_j\big)+...+\beta_m x_m+e \\
	& =\beta_0+\sum_{k=1}^m \beta_k x_k+\underbrace{\beta_j(\lambda-1)}_{\beta_{m+1}(\lambda)}x_j\ln x_j+e
\end{align*}
máme lineární model pro parametry $\beta_k$, $0\leq k\leq m+1$, protože $\beta_{m+1}(\lambda-1)\beta_j$ můžeme $(\lambda,\beta_j)$ odhadnout následovně:
\begin{enumerate}[1)]
	\item naladíme původní model a spočteme $\LSE$ $\wbeta_j$ parametru $\beta_j$,
	\item naladíme rozšířený model s $x_{m+1}=x_j\ln x_j$ a spočteme $\wbeta_{m+1}$,
	\item z rovnosti $\wbeta_{m+1}=(\widehat{\lambda}-1)\wbeta_j$ dostaneme $\widehat{\lambda}=\frac{\wbeta_{m+1}}{\wbeta_j}+1$.
\end{enumerate}
Tento postup umožňuje testovat potřebu transformace
$$ \hypothesis{\lambda=1}{\lambda\neq1}$$
pomocí t-testu pro $H_0:~\beta_{m+1}=0$.

\begin{remark}
Pokud model s $\widehat{\lambda}$ vypadá neadekvátně, lze postupovat iterativně a získat posloupnost $\widehat{\lambda}(l),~l\geq1$, $\widehat{\lambda}(0)=\widehat{\lambda}$ a rozvineme $x_j^\lambda$ kolem $\widehat{\lambda}(0)$, tzn. \\ $x_j^\lambda\approx x_j^{\widehat{\lambda}(0)}+\big(\lambda-\widehat{\lambda}(0)\big)x_j^{\widehat{\lambda}(0)}\ln x_j$ dosazením do rovnice modelu 
$$
Y= \beta_0=\sum_{\substack{k=1 \\ k\neq j}}^m \beta_k x_k+\beta_j x_j^{\widehat{\lambda}(0)}+\underbrace{\beta_j\big(\lambda-\widehat{\lambda}(0)\big)}_{\beta_{m+1}}x_j^{\widehat{\lambda}(0)}\ln x_j+e
$$ 
naladíme tento model s a bez přidané proměnné $x_{m+1}=x_j^{\widehat{\lambda}(0)}\ln x_j$. Označíme $\wbeta_j(1)$ a $\wbeta_{m+1}(1)$ příslušné odhady. Potom
$$
\widehat{\lambda}(1)=\widehat{\lambda}(0)+\frac{\wbeta_{m+1}(1)}{\wbeta_j(1)}.
$$
Můžeme dále iterovat do konvergence nebo skončit po pevném počtu iterací.
\end{remark}

\begin{remark}
	Další užívané transformace v $\x,~\Y$:\begin{description}
	\item[a) centrované proměnné:] $\X_C$ tak, že $(\X_C)_{ij}=x_{ij}-\overline{x}_j,~i\in\hat{n},~j\in\hat{m}$, kde $\overline{x}_j=\frac{1}{n}\sumin x_{ij}$ je průměr $j$-tého sloupce matice $\X$. $\Y_C$, $(\Y_C)_i=y_i-\overline{y}$... nevím
	\item[b) centrované a škálované proměnné:] škálování sloupců tak, aby jejich norma byla 1, tzn. každý prvek $j$-tého sloupce matice $\X$ podělíme $s_j=\Big(\sumin(x_{ij}-\overline{x}_j)^2\Big)^{\frac{1}{2}}$. Centrované a škálované matice $\X_\mathrm{SC}$ pak bude 
	$$
	\X_\mathrm{SC}=\X_C\textbf{S},\qquad \textbf{S}=\mathrm{diag}\Big(\frac{1}{s_1},...,\frac{1}{s_m}\Big).
	$$
	Model pak bude 
	$$
	\Y_C=\X_\mathrm{SC}\betab_s+e.$$ Lze použít i $\Y_\mathrm{SC}$, tedy centrované a škálované $\Y$.
	\end{description}
\end{remark}

\subsection*{Vážené nejmenší čtverce (weight least squares WLS)}
Budeme nyní předpokládat, že chyby $e_i$ jsou normální, nezávislé, ale $\D(e_i)=\sigma_i^2$ závisí na $i$. Konkrétně tedy $\sigma_i^2=\frac{\sigma^2}{w_i}$, kde $w_i>0,~i\in\hat{n}$ se nazývají váhy.

Uvažujeme tedy model 
\begin{equation}\label{rovnickaW}
\Y=\X\betab+\eb,\text{ kde }\eb\sim\NN_n(0,\sigma^2\textbf{W})\text{ a }\textbf{W}=\mathrm{diag}\Big(\frac{1}{w_1},...,\frac{1}{w_n}\Big).
\end{equation}
Pokud jsou váhy $w_i$ známé, lze MLE odhady parametru $\betab$ a $\boldsymbol{\sigma}^2$ nalézt následovně:
$$
\textbf{W}=\textbf{K} \textbf{K}^T,\text{ kde }\textbf{K}=\textbf{W}^{\frac{1}{2}}=\mathrm{diag}\Big(\frac{1}{\sqrt{w_1}},...,\frac{1}{\sqrt{w_n}}\Big).
$$
Definujeme $\textbf{Z}=\inv{\textbf{K}}\Y,~\textbf{M}=\inv{\textbf{K}}\X$ a $\boldsymbol{\epsilon}=\inv{\textbf{K}}\eb$. Potom dostaneme model 
\begin{equation}\label{rovnickaW2}
\textbf{Z}=\textbf{M}\betab+\boldsymbol{\epsilon},\text{ kde }\boldsymbol{\epsilon}\sim\NN_n(0,\sigma^2 I_n),
\end{equation}
protože
$$
\Cov(\boldsymbol{\epsilon})=\inv{\textbf{K}}\sigma^2\textbf{W}\big(\inv{\textbf{K}}\big)^T=\sigma^2 \inv{\textbf{K}}\textbf{K}\textbf{K}^T\big(\textbf{K}^T\big)^{-1}=\sigma^2 I_n.
$$

Transformační vektor je tedy ve tvaru $\textbf{Z}=(\sqrt{w_1}Y_1,...,\sqrt{w_n}Y_n)^T$. To už je standardní model LR, na kterém platí
$$ \wbetab_w=\inv{(\textbf{M}^T\textbf{M})}\textbf{M}^T\textbf{z}=\inv{\big(\X^T\underbrace{(\textbf{K}^{-1})^T \inv{\textbf{K}}}_{=\inv{{(\textbf{K}\textbf{K}^T)}}=\inv{\textbf{W}}}\X\big)}\X^T(\inv{\textbf{K}})^T\inv{\textbf{K}}\Y=\inv{(\X^T\inv{\textbf{W}}\X)}\X^T\inv{\textbf{W}}\Y.$$
Dále pak 
$$\widehat{{\sigma}^2}_w=\frac{1}{n}\sumin (z_i-\widehat{z}_i)^2=\frac{1}{n}\sumin w_i(y_i-\hyi)^2=\frac{1}{n}\SSE_w,$$ kde $\SSE_w$ je vážený součet čtverců, $z_i=\sqrt{w_i}y_i$ a $\widehat{z}_i=\sqrt{w_i}\x_i^T\wbetab=\sqrt{w_i}\hyi$.
Dále platí, že
\begin{enumerate}[a)]
	\item  $\E\wbetab_w=\inv{(\X^T\inv{\textbf{W}}\X)}\X^T\inv{\textbf{W}}\underbrace{\E \Y}_{\X\betab}=\wbeta$, kde $\wbetab_w$ je nestranný odhad $\wbeta$,
	\item $\E\Big(\frac{\SSE_W}{n-m-1}\Big)=\sigma^2$, tedy $s_w^2=\frac{\SSE_w}{n-m-1}$ je nestranný odhad $\sigma^2$.
\end{enumerate}

\begin{theorem}
Nechť $\wbetab_w$ je WLS odhad $\wbeta$, jestliže $\Cov(\eb)=\sigma^2\textbf{W}=\sigma^2\mathrm{diag}\Big(\frac{1}{w_1},...,\frac{1}{w_n}\Big)$. Potom platí, že
\begin{enumerate}[1)]
	\item $\Cov(\wbetab_w)=\sigma^2\inv{(\X\inv{\textbf{W}}\X)}$,
	\item nechť $\delta_i$ je $i$-tý diagonální prvek $\inv{(\X^T\inv{\textbf{W}}\X)}$. Jestliže $e_i\sim \NN\Big(0,\frac{\sigma^2}{w_i}\Big)$, $i\in\hat{n}$, potom 
	$$ T_i=\frac{\wbeta_{w,i}-\beta_i}{s_w\sqrt{\delta_i}}\sim t(n-m-1),$$
	\item pro $\widehat{\Y}=\X\wbeta_w$ platí, že $\E \widehat{\Y}_m=\X\textbf{B}$ a $\Cov(\widehat{\Y}_w)=\sigma^2\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T$,
\item nechť $\he_w=\Y-\widehat{\Y}_m$ jsou rezidua v modelu \ref{rovnickaW} a $\widehat{\boldsymbol{\epsilon}}_w=\textbf{Z}-\widehat{\textbf{Z}}=\textbf{Z}-\mathbb{M}\wbetab_w$ jsou rezidua v transformovaném modelu \ref{rovnickaW2}. Potom
$$ \widehat{\boldsymbol{\epsilon}}_w=\sqrt{\inv{\textbf{W}}}\he_w=\textbf{W}^{-\frac{1}{2}}\he_w\quad\text{a}\quad \E(he_w)=\E(\widehat{\epsilon_w})=0,$$
\item nechť $\textbf{H}_w=\X\inv{(\X^T\inv{\textbf{W}\X})}\X^T\inv{\textbf{W}}$ je vážená projekční matice. Potom
$$ \heb_w=(I-\textbf{H}_w)\eb\quad\text{a}\quad \Cov(\heb_w)=\sigma^2(I-\textbf{H}_w)\textbf{W}.$$
To znamená, že $$ \Cov(\widehat{\boldsymbol{\epsilon}_w})=\sigma^2\textbf{W}^{-\frac{1}{2}}(I-\textbf{H}_w)\textbf{W}^{\frac{1}{2}}.$$
\end{enumerate}

\begin{proof}
	\begin{enumerate}[1)]
		\item $$\Cov (\wbetab_w)=\inv{(\X^T\inv{\textbf{W}}\X)}\X^T\inv{\textbf{W}}\underbrace{\Cov \Y}_{=\sigma^2\textbf{W}}\inv{\textbf{W}}\X\inv{(\X^T\inv{\textbf{W}}\X)}=\sigma^2\inv{(\X^T\inv{\textbf{W}}\X)},$$
		\item $\D \wbeta_{w,i}=\sigma^2\delta_i$, tzn. $\frac{\wbeta_{w,i}-\beta_i}{\sigma\sqrt{\delta_i}}\sim\NN(0,1)$ a víme, že $\wbeta_{w,i}$ a $s_w^2$ jsou nezávislé, $\frac{s_w^2(n-m-1)}{\sigma^2}\sim\chi^2(n-m-1)$. Z toho vyplývá, že 
		$$ \frac{\wbeta_{w,i}-\beta_i}{s_w\sqrt{\delta_i}}\sim t(n-m-1).$$
		\item $\E \widehat{\Y}_w=\X\E \wbetab_w=\X\wbeta$, $\Cov(\widehat{\Y}_w)=\X\Cov\wbetab_w \X^T=\sigma^2\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T$.
		\item Protože $\textbf{Z}=\textbf{W}^{-\frac{1}{2}}\Y$ a $\textbf{M}=\textbf{W}^{-\frac{1}{2}}\X$, dostaneme 
		\[
		\begin{split}
		& \widehat{\boldsymbol{\epsilon}}_w=\textbf{Z}-\widehat{\textbf{Z}}=\textbf{W}^{-\frac{1}{2}}\Y-\textbf{W}^{-\frac{1}{2}}(\Y-\X\wbetab_w)=\textbf{W}^{-\frac{1}{2}\he_w},\\
		& \E \heb_w=\E(\Y-\widehat{\Y}_w)=\X\betab-\X\betab=\textbf{0}\quad\Rightarrow\quad \E \widehat{\boldsymbol{\epsilon}}=0.
		\end{split}
		\]
		\item \[
		\begin{split}
		\heb_w&=\Y-\widehat{\Y}_w=\Y-\X\wbetab_w=\X\betab+\eb-\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T\inv{\textbf{W}}\underbrace{(\X\betab+\textbf{e})}_{\Y}=\\&=\X\betab-\X\betab+\textbf{e}-\underbrace{\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T\inv{\textbf{W}}}_{\textbf{H}_w}\textbf{e}=(I-\textbf{H}_w)\textbf{e},\\
		\Cov(\heb_w)&=(I-\textbf{H}_w)\Cov~ \eb(I-\textbf{H}_w)^T=\\&=\sigma^2(I-\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T\inv{\textbf{W}})\textbf{W}\big(I-\inv{\textbf{W}}\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T\big)=\\&=\sigma^2\textbf{W}-\sigma^2\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T-\sigma^2\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T
+\sigma^2\X\inv{(\X^T\inv{\textbf{W}}\X)}\X^T=\\&=\sigma^2(I-\textbf{H}_w)\textbf{W}.	\end{split}
		\]
		$\Cov(\widehat{\boldsymbol{\epsilon}})=\textbf{W}^{-\frac{1}{2}}\Cov(\widehat{\boldsymbol{\epsilon}})\textbf{W}^{-\frac{1}{2}}=\sigma^2\textbf{W}^{-\frac{1}{2}}(I-\textbf{H}_w)\textbf{W}^{\frac{1}{2}}$.
	\end{enumerate}
\end{proof}
\end{theorem}

\begin{enumerate}[A)]

\item
Mallows $C_p$, AIC, BIC

Kritéria beroucí více v poraz počet použitých regresorů. Lze je použít i pro \textbf{nevnořené modely}!
\begin{itemize}
\item Mallows $C_p$
$$
C_p = \frac{\SSE_p}{\wsigma^2} - n + 2p, \quad \wsigma^2 = \frac{\SSE_T}{n-T}
$$
\textbf{Vlastnosti $C_p$}:
\begin{enumerate}[1)]
\item Snadno se počítá, $\SSE_p$ a $\wsigma^2$ jsou implementované.
\item Pokud je $\wsigma^2$ konzistentní odhad $\sigma^2$ (nazávisející na $p$), má $C_p$ následující interpretaci: Porovnává, co zbývá vysvětlit pomocí modelů s $p$ a $T$ parametry, zvýhodňuje počet dostuoných dat a penalizuje počet parametrů, které je třeba odhadnout.
\item Při zvyšování počtu regresorů: $q|sigma^2$ je konst., $\SSE_p$ klesá, $p$ roste ($C_p$ se snaží sladit dvě protichůdná kritéria).
\item $C_T = T$.
\item Pokud je správný model s $p$ parametry, dá se ukázat, že $C_p \approx p$ pro $n >> T$.
\item V praxi se volí model s nejmenším $C_p$ ve skupině modelů. (Obrázek)
\end{enumerate}

\begin{remark}
Pro dobrou interpretaci je třeba spočítat $C_p$ pro všechny nebo většinu podmnožin regresorů.
\end{remark}

\item
Akaikeho informační kritérium AIC

\newcommand{\AIC}{\mathrm{AIC}}
Obecná definice je
$$
\AIC = -2l(\wtheta) + 2p^*,
$$
kde $\wtheta$ je MLE odhad v modelu, $l$ je log-věrohodnostní funkce a $p^*$ je počet parametrů, které je třeba odhadnout ($p^* = p + 1$, jelikož počítáme i $\sigma^2$).

Pro náš model LR:
\begin{align*}
L(\betab,\sigma^2) & = \frac{1}{\left(2\pi \sigma^2\right)^{n/2}} \exp \left[ - \frac{1}{2\sigma^2} \left( \y - \X \betab \right)^T \left( \y - \X \betab \right) \right] \\
l(\betab, \sigma^2) & = - \frac{n}{2} \ln 2\pi - \frac{n}{2} \ln \sigma^2 - \frac{1}{2} \frac{\left( \y - \X \betab \right)^T \left( \y - \X \betab \right)}{\sigma^2} \\
\AIC & = -2l(\wbetab, \wsigma^2) + 2p^* = n \ln 2\pi + \ln \wsigma^2 + \frac{\left( \y - \X \betab \right)^T \left( \y - \X \betab \right)}{\wsigma^2} + 2p^*,
\end{align*}
ale $\wsigma^2 = \frac{1}{n} \left( \y - \X \betab \right)^T \left( \y - \X \betab \right) = \frac{\SSE}{n}$, tedy
$$
\AIC = \underbrace{n \ln 2 \pi + n}_{C} + n \ln \frac{\SSE}{n} + 2p^*
$$
nebo alternativně $\AIC = n \ln \frac{\SSE}{n} + 2p^*$.

\begin{remark}
\begin{itemize}
\item hledáme model s minimální hodnotou AIC
\item AIC není mírou kvality modelu, je užitečná pro porovnávání modelů
\end{itemize}
\end{remark}

\textbf{AIC v R:}

- \verb|AIC(.)| počítá $\AIC = n \ln 2 \pi + n + n \ln \frac{\SSE}{n} + 2p^*$, kde $p^*$ je počet parametrů $\betab, \sigma^2$ (včetně interceptu) \\
- \verb|extractAIC(.)| počítá $\AIC =  + n \ln \frac{\SSE}{n} + 2p^*$, kde $p^*$ je jen počet parametrů $\betab$ (včetně interceptu)

\item 
(Schwarzovo) bayesovské informační kritérium BIC

\newcommand{\BIC}{\mathrm{BIC}}

Z definice
$$
\BIC = -2 l(\wtheta) + p^* \ln n.
$$
Více penalizuje počet parametrů $\implies$ vybírá jednodušší modely s jednodušší interpretací než AIC. BIC vyžaduje významnější příspěvek proměnné, aby byla zařazena do modelu. (AIC se zanořuje za predikci, BIC je kompromis mezi interpretovatelností a predikcí.)

\textbf{BIC v R:}

- \verb|BIC(.)| nebo \verb|AIC(.), extractAIC(.)| s volbou \verb|k = log(nobs(fit))|.

Příklad - data HALD.
\end{itemize}

\item 
PRESS statistika

\newcommand{\PRESS}{\mathrm{PRESS}}

Pokud je pro nás důležitá kvalita predikve, lze použít pro srovnání modelů statistiku
$$
\PRESS = \sumin \he_{(-i)}^2 = \sumin \left(  \frac{\hei}{1 - \hii} \right)^2.
$$
Vybírá se model s minimální hodnotou této statistiky.
\end{enumerate}

\section{Metody výběru modelu}

\begin{enumerate}[1)]

\item
\textbf{Vyhodnocení všech možných modelů}

Pro $T$ dostupných regresorů tzn. naladit $2^T$ modelů. Pak je porovnat pomocí nějakého kritéria. Náročné pro velká $T$ (například $T = 20$ znamená $1024$ modelů).

\item
\textbf{Zpětná eliminace (backward elimination)}

Začneme s plným modelem a v každém kroku odstraníme jednu proměnnou, která nejméně přispívá modelu (měřeno F stat) nebo jejíž odstranění znamená největší zlepšení modelu (měřeno AIC).

\textbf{algoritmus:}
\begin{enumerate}[	1)]
	\item Naladíme model se všemi proměnnými.
	\item Pro každou proměnnou spočteme částečnou $\FF$ statistiku (nebo $t$-statistiku) jako by právě byla přidána do modelu, tzn. za předpokladu, že ostatní proměnné tam už jsou.
	\item Pokud je nějaká $\FF$-statistika menší, než kritická hodnota $\FF_\mathrm{OUT}$, vynecháme z modelu proměnnou s nejnižší hodnotou $\FF$. $\FF_\mathrm{OUT}=\FF_{1-\alpha_\mathrm{OUT}}(1,n-p)$, kde $p$ je aktuální počet regresorů v modelu, včetně interceptu, $\alpha_\text{OUT}=0.05,0.1,...$
	\item Opakujeme kroky 2) a 3), dokud všechny částečné $\FF$ statistiky nejsou větší, než příslušná kritická hodnota $\FF_\mathrm{OUT}$, tzn. nelze už vyřadit žádnou proměnnou.
\end{enumerate}
\begin{remark}
	Místo $\FF$ lze foužívat AIC.
\end{remark}

\item
\textbf{Dopředná regrese (forward regression)}

Začneme pouze s interceptem (nebo nutným minimálním modelem) a v každém kroku přidáme jednu proměnnou, která má za následek největší zlepšení modelu (největší nárůst $\FF$ nebo největší pokles AIC). 

Tato metoda neumožňuje odstranit proměnnou, která už do modelu byla přidána.

\textbf{algoritmus:}\begin{enumerate}[	1)]
	\item Naladíme minimální model.
	\item Pro každou dostupnou proměnnou spošteme $\FF$ statistiku pro test významnosti jijího přidání do modelu.
	\item Pokud některá z těchto $\FF$ statistik překračuje kritickou hodnotu $\FF_\mathrm{in}$, přidáme do modelu proměnnou s nejvyšší hodnotou $\FF$ statistiky.
	\item Opakujeme kroky 2) a 3), dokud všechny $\FF$-statistiky pro zbývající proměnné nebudou menší, než $\FF_\mathrm{in}$ nebo dokud nezbyde žádná proměnná na přidání do modelu.
\end{enumerate}
\begin{remark}
	I když tento postup zjednodušuje výběr modelu, často bohužel vede na zařazení proměnných, které nemají významný příspěvek, jakmile jsou zařazeny další proměnné.
\end{remark}

\item
\textbf{Postupná regrese (stepwise regression)}

Kombinace dvou předchozích metod. V každém kroku algoritmu přidáváme jednu proměnnou a poté zkontrolujeme, zda není možné nějakou odebrat. Budeme potřebovat  dvě kritické hodnoty $\FF_\mathrm{in}$, $\FF_\mathrm{OUT}$ (pro použití $\FF$ statistiky).

\textbf{algoritmus:}\begin{enumerate}[	1)]
	\item Naladíme minimální model.
	\item Zjistíme, zda přidání nějaké další proměnné může zlepšit model ($\FF$ nebo AIC). Pokud ano, přidáme do modelu proměnnou, která má za následek největší zlepšení modelu (nebo největší pokles AIC).
	\item V novém modelu zjistíme, zda nelze některou proměnnou vynechat (opět pomocí AIC nebo $\FF$). Pokud ano, vynecháme proměnnou, jejíž vyřazení má za následek největší zlepšení modelu (nebo největší pokles AIC).
	\item Opakujeme kroky 2) a 3) do té doby, až nebude možné přidat ani ubrat žádnou proměnnou.
\end{enumerate}

\textbf{Princip marginality:}
\begin{enumerate}[	1)]
	\item Pokud jsou v modelu vyšší mocniny nějakého regressoru, měly by tam být obsaženy i všechny jeho nižší mocniny (i když jsou případně nevýznamné).
	\item Pokud je v modelu obsažena interakce dvou regressorů, měly by tam být i oba individuální regresory.
	\item S každou interakcí vyššího řádu by měl model obsahovat i všechny interakce řádu nižšího. ($a:b:c~\to~a:b,a:c,b:c$).
\end{enumerate}

\begin{remark}
	Jakmile nalezneme optimální model, je třeba řádně ověřit adekvátnost.
\end{remark}

\end{enumerate}

